#!/usr/bin/env python3

# Requirements:
# --- for Debian,Ubuntu series ---
# apt-get install python3 python3-docopt python3-schema
# (optional) python3-tqdm
#
# --- for RHEL,CentOS series ---
# yum install     epel-release
# yum install     python3 python3-docopt python3-schema
# (optional) python3-tqdm
#
# --- Or, install to ~/.local just for user's environment ---
# (required: python3-pip)
# pip3 install --user docopt schema tqdm

# Coding style check:
# flake8 ./gfptar

import os
import errno
import stat
import sys
import pwd
import grp
import time
import re
import abc
import tarfile
from pprint import pformat
import logging
import subprocess
import concurrent.futures
import threading
from typing import NoReturn
from urllib.parse import urlparse
import shutil
from contextlib import contextmanager
import traceback
import unittest
import hashlib

from docopt import docopt
from schema import Schema, Use, Or

try:
    from tqdm import tqdm
    have_tqdm = True
except Exception:
    have_tqdm = False


# library
def unhumanize_number(numstr):
    strlen = len(numstr)
    if strlen == 1:
        return int(numstr)
    n = int(numstr[:(strlen-1)])
    si_prefix = numstr[(strlen-1):]
    prefixes = {'K': 1,
                'M': 2,
                'G': 3,
                'T': 4,
                'P': 5,
                'E': 6,
                }
    power = prefixes.get(si_prefix.upper())
    if power is None:
        return int(numstr)
    return n * (1024 ** power)


class GfLogger(logging.getLoggerClass()):
    def __init__(self):
        super().__init__()
        self.myinit()

    def myinit(self):
        if getattr(self, 'lock', None) is None:
            self.lock = threading.Lock()

    # REFERENCE: logging/__init__.py: class Logger()
    def debug(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.DEBUG):
            with self.lock:
                self._log(logging.DEBUG, msg, args, **kwargs)

    def info(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.INFO):
            with self.lock:
                self._log(logging.INFO, msg, args, **kwargs)

    def warning(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.WARNING):
            with self.lock:
                self._log(logging.WARNING, msg, args, **kwargs)

    def error(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.ERROR):
            with self.lock:
                self._log(logging.ERROR, msg, args, **kwargs)

    def error_exit(self, exit_code, msg, *args, **kwargs):
        if self.isEnabledFor(logging.ERROR):
            with self.lock:
                self._log(logging.ERROR, msg, args, **kwargs)
        sys.exit(exit_code)

    def fatal(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.ERROR):
            with self.lock:
                self._log(logging.ERROR, msg, args, **kwargs)
            if 'exit_code' in kwargs:
                raise Exception('exit_code={}'.format(kwargs['exit_code']))
            else:
                raise Exception('fatal exit')


logger = None


def logger_init(name, loglevel=logging.WARNING, debug=False):
    global logger

    if logger is not None:
        return logger

    logger = logging.getLogger()  # RootLogger
    logger.__class__ = GfLogger
    logger.myinit()
    logger.setLevel(loglevel)
    strm = logging.StreamHandler()  # stderr
    if debug:
        fmt = '{}:%(levelname)s:L%(lineno)d:%(asctime)s: %(message)s'.format(
            name)
    else:
        fmt = '{}:%(levelname)s: %(message)s'.format(name)
    formatter_strm = logging.Formatter(fmt=fmt, datefmt='%Y%m%d%H%M%S')
    strm.setFormatter(formatter_strm)
    strm.setLevel(loglevel)
    logger.addHandler(strm)
    return logger


_encoding = sys.getfilesystemencoding()


def get_encoding():
    return _encoding


def set_encoding(enc):
    global _encoding
    if _encoding != enc:
        _encoding = enc


def execcmd_raw(args, stdin=subprocess.DEVNULL, stderr=subprocess.PIPE,
                timeout=None, textmode=False):
    if textmode:
        encoding = get_encoding()
    else:
        encoding = None
    proc = subprocess.Popen(
        args, shell=False, encoding=encoding, close_fds=True,
        stdin=stdin, stdout=subprocess.PIPE, stderr=stderr)
    try:
        out, err = proc.communicate(timeout=timeout)
    except subprocess.TimeoutExpired:
        proc.kill()
        out, err = proc.communicate()
    ret = proc.wait()
    return out, err, ret


def execcmd(args, stdin=subprocess.DEVNULL, stderr=subprocess.PIPE,
            timeout=None, textmode=False):
    out, err, ret = execcmd_raw(args, stdin=stdin, stderr=stderr,
                                timeout=timeout, textmode=textmode)
    if ret != 0:
        try:
            err = err.decode().rstrip()
        except Exception:
            pass
        raise Exception('{}: {}'.format(str(args), err))
    if err:
        logger.warning(err)
    return out


# text mode
def execcmd_readline(args, stdin=subprocess.DEVNULL, stderr=sys.stderr.buffer):
    proc = subprocess.Popen(
        args, shell=False, encoding=get_encoding(), close_fds=True,
        stdin=stdin, stdout=subprocess.PIPE, stderr=stderr)
    while True:
        line = proc.stdout.readline()
        if line:
            line = line.rstrip('\r\n')
            yield line
        elif proc.poll() is not None:
            break
    ret = proc.wait()
    if ret != 0:
        raise Exception('{}: returncode={}'.format(' '.join(args), ret))


class Command(metaclass=abc.ABCMeta):
    def init(self, name) -> NoReturn:
        self._docopt = docopt(self.getDoc())
        self.opt = self.getSchema().validate(self._docopt)
        self.debug = self.opt['--debug']
        self.verbose = self.opt['--verbose']
        self.quiet = self.opt['--quiet']

        loglevel = logging.WARNING
        if self.debug:
            loglevel = logging.DEBUG
        elif self.verbose:
            loglevel = logging.INFO
        elif self.quiet:
            loglevel = logging.ERROR
        self.loglevel = loglevel

        # use stderr
        mylogger = logger_init(name, loglevel=loglevel, debug=self.debug)
        self.logger = mylogger

        logger.debug('USE_GFMKDIR_PLUS: %s', USE_GFMKDIR_PLUS)
        logger.debug('USE_GFCHMOD_PLUS: %s', USE_GFCHMOD_PLUS)
        logger.debug('USE_GFREG_PLUS: %s', USE_GFREG_PLUS)

    @abc.abstractmethod
    def getDoc(self) -> str:
        raise NotImplementedError

    @abc.abstractmethod
    def getSchema(self) -> Schema:
        raise NotImplementedError

    @abc.abstractmethod
    def run(self, option) -> NoReturn:
        raise NotImplementedError


class GfURLEntry():
    TYPE_FILE = 'FILE'
    TYPE_DIR = 'DIR'
    TYPE_SYMLINK = 'SYM'
    TYPE_OTHER = 'OTHER'

    def __init__(self, start_url, path, mode, file_type, uname, gname,
                 size, mtime, linkname):
        self.start_url = start_url  # GfURL
        self.path = path
        self.mode = mode
        self.file_type = file_type
        self.uname = uname
        self.gname = gname
        self.size = size
        self.mtime = mtime
        self.linkname = linkname

        if not self.is_file():
            self.size = 0

    def __str__(self):
        return f'{self.path}, {self.mode:o}, {self.file_type}'

    def __repr__(self):
        return str(self)

    def subpath(self, baseurl):
        return baseurl.subpath(self.path)

    def is_file(self):
        return self.file_type == self.TYPE_FILE

    def is_directory(self):
        return self.file_type == self.TYPE_DIR

    def is_symlink(self):
        return self.file_type == self.TYPE_SYMLINK

    def toTarinfo(self, path):
        tarinfo = tarfile.TarInfo(path)
        if self.is_file():
            tarinfo.type = tarfile.REGTYPE
        elif self.is_directory():
            tarinfo.type = tarfile.DIRTYPE
        elif self.is_symlink():
            tarinfo.type = tarfile.SYMTYPE
        else:
            logger.warning('unsupported type: %s, %s', path, self.file_type)
            return None
        tarinfo.mode = self.mode
        tarinfo.mtime = self.mtime
        tarinfo.size = self.size
        tarinfo.linkname = self.linkname
        tarinfo.uname = self.uname
        tarinfo.gname = self.gname
        return tarinfo

    def compare(self, ent2, data=False, same_owner=False, bufsize=1048576):
        ent1 = self

        def cmpval(a, b, name):
            if name == 'mtime':
                if USE_GFMKDIR_PLUS is False \
                   or USE_GFCHMOD_PLUS is False \
                   or USE_GFREG_PLUS is False:
                    # skip (not supported)
                    return
                a = int(a)
                b = int(b)
            elif name == 'mode':
                a = oct(a & 0o7777)
                b = oct(b & 0o7777)
            if a != b:
                raise Exception(
                    f'{ent1.path} vs {ent2.path}: prop={name}: {a} != {b}')
            logger.debug(f'GfURLEntry.compare:prop={name}: PASS')

        def cmpprop(ent1, ent2, properties):
            for pname in properties:
                val1 = getattr(ent1, pname)
                val2 = getattr(ent2, pname)
                cmpval(val1, val2, pname)

        # path is not compared
        cmpprop(ent1, ent2, ['file_type', 'mode', 'mtime', 'size', 'linkname'])
        if same_owner:
            cmpprop(ent1, ent2, ['uname', 'gname'])
        if data and ent1.is_file():
            url1 = GfURL.init(ent1.path)
            url2 = GfURL.init(ent2.path)
            if not url1.compare_data(url2, bufsize=bufsize):
                raise Exception(f'{ent1.path} vs {ent2.path}: different data')
            logger.debug('GfURLEntry.compare:data: PASS')
        return True


class GfURL(metaclass=abc.ABCMeta):
    MAXNAMLEN = 255  # SEE ALSO: dirent.h, gfarm/gfs.h (GFS_MAXNAMLEN)

    @staticmethod
    def init(url, use_gfarm_command=False, local=False):
        if local:
            return GfURLLocal(url)
        if GfURLGfarm.is_my_URL(url):
            return GfURLGfarm(url)
        gfurl1 = GfURLLocal(url)
        if not use_gfarm_command:
            return gfurl1
        # use_gfarm_command=True: use gf* commands on gfarm2fs
        gfurl2 = gfurl1.get_gfarm_url_by_gfarm2fs()
        if gfurl2 is not None:
            # replace
            logger.debug('use Gfarm URL(%s) on gfarm2fs', gfurl2.url_str)
            return gfurl2
        return gfurl1

    def __init__(self, url):
        self._url = urlparse(url)

    @property
    def basename(self):
        return os.path.basename(self.path)

    @property
    def parent(self):
        parent_path = os.path.dirname(self.path)
        # "." ... relative path (of local path)
        # "/" ... absolute path
        if self.root_url_str != '' \
           and (parent_path == '.' or parent_path == '/'):
            parent_path = ''
        return GfURL.init(self.root_url_str + parent_path)

    @property
    def parent_list(self):
        def parents_func(u):
            if u.path != '.' and u.path != '/':
                yield u.parent
                yield from parents_func(u.parent)
                return
        yield from parents_func(self)

    @property
    def url_str(self):
        return self._url.geturl()

    @property
    def path(self):
        return os.path.normpath(self._url.path)

    @property
    def root_url_str(self):
        # ex. http://example.com/a/b/c -> http://example.com
        return self.url_str[:-len(self._url.path)]

    def subpath(self, fullpath):
        base = self.url_str
        if not fullpath.startswith(base):
            logger.error('subpath: %s, %s', base, fullpath)
            raise AssertionError
        logger.debug('subpath: %s, %s', base, fullpath)
        return fullpath[len(base):].lstrip('/')  # relative path

    def url_join(self, subpath):
        return os.path.join(self.url_str, subpath.lstrip('/'))

    def is_gfarm(self):
        return isinstance(self, GfURLGfarm)

    def is_local(self):
        return isinstance(self, GfURLLocal)

    def create_new_dir(self):
        if self.exists():
            raise FileExistsError(self.url_str)
        self.mkdir()
        if not self.is_directory():
            raise NotADirectoryError(self.url_str)
        if not self.is_empty():
            raise FileExistsError(self.url_str)

    @classmethod
    @abc.abstractmethod
    def is_my_URL(cls, url):
        raise NotImplementedError

    @abc.abstractmethod
    def chmod(self, mode, mtime=None, user=None, group=None,
              follow_symlinks=True):
        raise NotImplementedError

    @abc.abstractmethod
    def chown(self, user, group, follow_symlinks=True):
        raise NotImplementedError

    @abc.abstractmethod
    def utime(self, atime, mtime, follow_symlinks=True):
        raise NotImplementedError

    @abc.abstractmethod
    def mkdir(self, mode=0o700, parents=False):
        raise NotImplementedError

    def makedirs(self, mode=0o700):
        self.mkdir(mode, parents=True)

    @abc.abstractmethod
    def remove_tree(self, remove_readonly=False):
        raise NotImplementedError

    @abc.abstractmethod
    def symlink(self, linkname):
        raise NotImplementedError

    @abc.abstractmethod
    def hardlink(self, linkname):
        raise NotImplementedError

    @abc.abstractmethod
    def exists(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_directory(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_symlink(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_empty(self):
        raise NotImplementedError

    @abc.abstractmethod
    def get_size(self):
        raise NotImplementedError

    @abc.abstractmethod
    def listdir(self, path_only=False, recursive=False, first=False,
                hardlink_warn=True):
        raise NotImplementedError

    @contextmanager
    @abc.abstractmethod
    def readopen(self, textmode=False):
        raise NotImplementedError

    @contextmanager
    @abc.abstractmethod
    def writeopen(self, textmode=False, mode=0o600, mtime=None,
                  user=None, group=None):
        raise NotImplementedError

    def tar_add(self, tar, subpath, entry):
        tarinfo = entry.toTarinfo(subpath)
        if tarinfo is None:
            return
        # hard link is not supported
        if entry.is_file():
            url = GfURL.init(entry.path)
            with url.readopen() as f:
                tar.addfile(tarinfo, fileobj=f)
        else:
            tar.addfile(tarinfo)

    def copy_from(self, inf, bufsize, mode=None, mtime=0o600,
                  user=None, group=None):
        readlen = 0
        with self.writeopen(mode=mode, mtime=mtime,
                            user=user, group=group) as outf:
            while True:
                buf = inf.read(bufsize)
                if not buf:
                    break
                # binary mode
                wlen = outf.write(buf)
                readlen += wlen
        return readlen

    def sha256(self, bufsize=1048576):
        h = hashlib.sha256()
        with self.readopen() as f:
            while True:
                buf = f.read(bufsize)
                if not buf:
                    break
                h.update(buf)
        return h.hexdigest()

    def compare_data(self, url2, bufsize=1048576):
        url1 = self
        return url1.sha256(bufsize) == url2.sha256(bufsize)

    def compare_entries(self, d2url, data=True, same_owner=False,
                        bufsize=1048576):
        def search_compare_pop(subpath, ent1, dirdict2):
            ent2 = dirdict2.pop(subpath, None)
            if ent2 is None:
                return False  # not found
            ent1.compare(ent2, data=data, same_owner=same_owner,
                         bufsize=bufsize)
            # raise if different
            return True

        d1url = self
        d1i = d1url.listdir(recursive=True)
        d2i = d2url.listdir(recursive=True)
        d1dict = dict()
        d2dict = dict()
        d2stop = False
        for d1ent in d1i:
            subpath1 = d1ent.subpath(d1url)
            logger.debug('compare:dir1: %s', subpath1)
            try:
                if not search_compare_pop(subpath1, d1ent, d2dict):
                    # not found, compare later
                    d1dict[subpath1] = d1ent
            except Exception as e:
                logger.error(str(e))
                return False
            while not d2stop:
                d2ent = next(d2i, None)
                if d2ent is None:
                    d2stop = True
                    break
                subpath2 = d2ent.subpath(d2url)
                logger.debug('compare:dir2: %s', subpath1)
                try:
                    if not search_compare_pop(subpath2, d2ent, d1dict):
                        # not found, compare later
                        d2dict[subpath2] = d2ent
                except Exception as e:
                    logger.error(str(e))
                    return False
        if d2stop is False:
            while True:
                d2ent = next(d2i, None)
                if d2ent is None:
                    break
                subpath2 = d2ent.subpath(d2url)
                logger.debug('compare:dir2: %s', subpath1)
                d2dict[subpath2] = d2ent
        d1notfound = dict()
        for subpath1, d1ent in d1dict.items():
            try:
                if not search_compare_pop(subpath1, d1ent, d2dict):
                    d1notfound[subpath1] = d1ent
            except Exception as e:
                logger.error(str(e))
                return False
        if len(d1notfound) > 0 or len(d2dict) > 0:
            logger.error('different entries: dir1=%s, dir2=%s',
                         pformat(d1notfound), pformat(d2dict))
            return False
        logger.debug('compare_entries: PASS')
        return True


def str2bool(s):
    return s.upper() in ['TRUE', '1', 'ON', 'ENABLE', 'ENABLED']


USE_GFMKDIR_PLUS = str2bool(os.getenv('GFMKDIR_PLUS', 'True'))
USE_GFCHMOD_PLUS = str2bool(os.getenv('GFCHMOD_PLUS', 'True'))
USE_GFREG_PLUS = str2bool(os.getenv('GFREG_PLUS', 'True'))


class GfURLGfarm(GfURL):
    # Ex. 12345 -rw-rw-r-- 1 user1  gfarmadm     29 Jan  1 00:00:00 2022 fname
    PAT_ENTRY = re.compile(r'^\s*(\d+)\s+([-dl]\S+)\s+(\d+)\s+(\S+)\s+(\S+)\s+'
                           r'(\d+)\s+(\S+\s+\d+\s+\d+:\d+:\d+\s+\d+)\s+(.+)$')
    PAT_EMPTY = re.compile(r'^\s*$')

    def __init__(self, url):
        super().__init__(url)

    @classmethod
    def is_my_URL(cls, url):
        return url.startswith('gfarm:')

    @staticmethod
    def to_oct_str(mode):
        # ex. (int)0o644 -> (str)'644'
        return oct(mode)[2:]

    def chmod(self, mode, mtime=None, user=None, group=None,
              follow_symlinks=True):
        args = ['gfchmod']
        if USE_GFCHMOD_PLUS:
            if mtime is not None:
                args.append('-M')
                args.append(str(int(mtime)))
            if user is not None:
                args.append('-u')
                args.append(user)
            if group is not None:
                args.append('-g')
                args.append(group)
        if not follow_symlinks:
            args.append('-h')
        args.append(self.to_oct_str(mode))
        args.append(self.url_str)
        execcmd(args)
        if not USE_GFCHMOD_PLUS:
            if mtime is not None:
                self.utime(mtime, mtime, follow_symlinks=follow_symlinks)
            self.chown(user, group, follow_symlinks=follow_symlinks)

    def chown(self, user, group, follow_symlinks=True):
        if user is None and group is None:
            return
        chown_args = ['gfchown']
        if not follow_symlinks:
            chown_args.append('-h')
        if user is not None and group is not None:
            chown_args.append(user + ':' + group)
        elif user is not None:
            chown_args.append(user)
        elif group is not None:
            chown_args.append(':' + group)
        chown_args.append(self.url_str)
        execcmd(chown_args)

    def utime(self, atime, mtime, follow_symlinks=True):
        # not used when USE_GFCHMOD_PLUS == True
        logger.warning('mtime cannot be updated (not supported): %s',
                       self.url_str)

    def mkdir(self, mode=0o700, parents=False):
        args = ['gfmkdir']
        if parents:
            args.append('-p')
        if USE_GFMKDIR_PLUS:
            if mode is not None:
                args.append('-m')
                args.append(self.to_oct_str(mode))
            args.append(self.url_str)
            execcmd(args)
        else:
            args.append(self.url_str)
            execcmd(args)
            if mode is not None:
                # slow
                self.chmod(mode)

    def remove_tree(self, remove_readonly=False):
        path = self.url_str
        if path == '/' or path == '':
            raise AssertionError
        try:
            execcmd(['gfrm', '-rf', path])
        except Exception:
            if remove_readonly:
                execcmd(['gfchmod', '-R', '700', path])
                execcmd(['gfrm', '-rf', path])
            else:
                raise

    def symlink(self, linkname):
        execcmd(['gfln', '-s', linkname, self.url_str])

    def hardlink(self, linkname):
        execcmd(['gfln', linkname, self.url_str])

    def exists(self):
        out, err, ret = execcmd_raw(['gftest', '-e', self.url_str])
        return ret == 0

    def is_directory(self):
        out, err, ret = execcmd_raw(['gftest', '-d', self.url_str])
        return ret == 0

    def is_symlink(self):
        out, err, ret = execcmd_raw(['gftest', '-h', self.url_str])
        return ret == 0

    def is_empty(self):
        count = 0
        for line in execcmd_readline(['gfls', '-1a', self.url_str]):
            if line is None:
                continue
            if line == '.' or line == '..':
                continue
            count += 1
        logger.debug('is_empty: %s, %d', self.url_str, count)
        return count == 0

    def get_size(self):
        for entry in self.listdir(recursive=False):
            logger.debug('get_size: %s, %d', entry.path, entry.size)
            return entry.size

    @classmethod
    def from_rwx(cls, rwx, highchar):
        perm = 0
        highbit = 0
        r = rwx[0]
        w = rwx[1]
        x = rwx[2]
        if r == 'r':
            perm |= 0o4
        if w == 'w':
            perm |= 0o2
        if x == 'x':
            perm |= 0o1
        elif x == highchar:
            perm |= 0o1
            highbit = 0o1
        elif x == highchar.upper():
            highbit = 0o1
        return perm, highbit

    @classmethod
    def mode_convert(cls, mode_str, name):
        typestr = mode_str[0]
        file_type = None
        if typestr == 'd':
            file_type = GfURLEntry.TYPE_DIR
        elif typestr == 'l':
            file_type = GfURLEntry.TYPE_SYMLINK
        elif typestr == '-':
            file_type = GfURLEntry.TYPE_FILE
        else:
            logger.warning('unsupported type: %s, %s', name, typestr)

        mode = 0
        perm, highbit = cls.from_rwx(mode_str[1:4], 's')
        mode |= (perm << 6)
        mode |= (highbit << 11)
        perm, highbit = cls.from_rwx(mode_str[4:7], 's')
        mode |= (perm << 3)
        mode |= (highbit << 10)
        perm, highbit = cls.from_rwx(mode_str[7:10], 't')
        mode |= perm
        mode |= (highbit << 9)
        return mode, file_type

    def listdir(self, path_only=False, recursive=False, first=False,
                hardlink_warn=False):
        dirname = self.url_str
        gfls_opt = '-ailT'
        if recursive:
            gfls_opt += 'R'
        for line in execcmd_readline(['gfls', gfls_opt, self.url_str]):
            logger.debug('listdir: raw line=%s', line)
            if line is None:
                continue
            if self.PAT_EMPTY.match(line):
                continue
            m = self.PAT_ENTRY.match(line)
            if m:
                # Ex.
                # 12345 -rw-rw-r-- 1 user1 group1 29 Jan 1 00:00:00 2022 fname
                inum = int(m.group(1))
                mode_str = m.group(2)
                nlink = int(m.group(3))
                uname = m.group(4)
                gname = m.group(5)
                size = int(m.group(6))
                mtime_str = m.group(7)
                name = m.group(8)

                if name == '..':
                    continue
                if not first and name == '.':
                    continue
                mtime = time.mktime(
                    time.strptime(mtime_str, '%b %d %H:%M:%S %Y'))
                mode, file_type = self.mode_convert(mode_str, name)
                if file_type is None:
                    continue
                linkname = ''
                if file_type == GfURLEntry.TYPE_SYMLINK:
                    link = name.split(' -> ')
                    name = link[0]
                    linkname = link[1]
                elif file_type == GfURLEntry.TYPE_FILE:
                    if hardlink_warn and nlink >= 2:
                        logger.warning('hard link is not supported: '
                                       'nlink=%d, inode=%d (Gfarm): %s',
                                       nlink, inum, name)
                if first and name == '.':
                    path = dirname
                else:
                    path = dirname + '/' + name
                # path is Gfarm URL (gfarm:/...)
                start = self
                yield GfURLEntry(start, path, mode, file_type, uname, gname,
                                 size, mtime, linkname)
            else:
                # ex. gfarm:/home/user1/dir: -> gfarm:/home/user1/dir
                dirname = line[:-1]
                if not dirname.startswith(self.url_str):
                    raise AssertionError
                first = False

    def gfreg(self, textmode=False, mode=None, mtime=None,
              user=None, group=None):
        class Gfreg():
            def __init__(self, url, proc, stdin):
                self.url = url
                self.proc = proc
                self.stdin = stdin

            def close(self):
                self.stdin.close()

            def post(self):
                if USE_GFREG_PLUS:
                    return
                # must be called after wait()
                if mode is not None:
                    self.url.chmod(mode, mtime=mtime, user=user, group=group)
                else:
                    if mtime is not None:
                        self.url.utime(mtime, mtime)
                    self.url.chown(user, group)

        args = ['gfreg']
        if USE_GFREG_PLUS:
            if mode is not None:
                args.append('-m')
                args.append(self.to_oct_str(mode))
            if mtime is not None:
                args.append('-M')
                args.append(str(int(mtime)))
            if user is not None:
                args.append('-u')
                args.append(user)
            if group is not None:
                args.append('-g')
                args.append(group)
        args.append('-')
        args.append(self.url_str)
        if textmode:
            encoding = get_encoding()
        else:
            encoding = None
        proc = subprocess.Popen(
            args, shell=False, encoding=encoding, close_fds=True,
            stdin=subprocess.PIPE, stdout=subprocess.DEVNULL,
            stderr=sys.stderr.buffer)
        return Gfreg(self, proc, proc.stdin)

    def gfexport(self, textmode=False):
        args = ['gfexport', self.url_str]
        if textmode:
            encoding = get_encoding()
        else:
            encoding = None
        return subprocess.Popen(
            args, shell=False, encoding=encoding, close_fds=True,
            stdin=subprocess.DEVNULL, stdout=subprocess.PIPE,
            stderr=sys.stderr.buffer)

    @contextmanager
    def readopen(self, textmode=False):
        proc = self.gfexport(textmode=textmode)
        try:
            yield proc.stdout
        finally:
            proc.stdout.close()
            ret = proc.wait()
            if ret != 0:
                raise Exception('{}: returncode={}'.format(
                    ' '.join(proc.args), ret))

    @contextmanager
    def writeopen(self, textmode=False, mode=0o600, mtime=None,
                  user=None, group=None):
        gfreg_obj = self.gfreg(textmode=textmode, mode=mode, mtime=mtime,
                               user=user, group=group)
        proc = gfreg_obj.proc
        try:
            yield gfreg_obj.stdin
        finally:
            gfreg_obj.close()
            ret = proc.wait()
            if ret != 0:
                raise Exception('{}: returncode={}'.format(
                    ' '.join(proc.args), ret))
            gfreg_obj.post()


def get_uid(user):
    try:
        pw = pwd.getpwnam(user)
        if pw is not None:
            return pw.pw_uid
    except KeyError:
        pass
    return -1  # uid is not changed by os.chown()


def get_gid(group):
    try:
        gr = grp.getgrnam(group)
        if gr is not None:
            return gr.gr_gid
    except KeyError:
        pass
    return -1  # gid is not changed by os.chown()


class GfURLLocal(GfURL):
    def __init__(self, url):
        super().__init__(url)

    @classmethod
    def is_my_URL(cls, url):
        return True

    def get_gfarm_url_by_gfarm2fs(self):
        # SEE ALSO: lib/libgfarm/gfarm/gfarm_path.c
        XATTR_GFARM2FS_URL = 'gfarm2fs.url'
        try:
            val = os.getxattr(self.url_str, XATTR_GFARM2FS_URL)
            gfurl = GfURL.init(val.decode(get_encoding()))
            if gfurl.is_gfarm():
                return gfurl
            raise Exception('unexpected Gfarm URL format: ' + gfurl.url_str)
        except OSError as e:
            if e.errno == errno.EOPNOTSUPP:
                return None
            elif e.errno == errno.ENOENT:
                # creation mode
                try:
                    val = os.getxattr(self.parent.url_str, XATTR_GFARM2FS_URL)
                    parent_str = val.decode(get_encoding())
                    gfurl = GfURL.init(os.path.join(parent_str, self.basename))
                    if gfurl.is_gfarm():
                        return gfurl
                    raise Exception('unexpected Gfarm URL format: '
                                    + gfurl.url_str)
                except OSError as e2:
                    if e2.errno == errno.EOPNOTSUPP:
                        # local filesystem
                        return None
                    raise
                except Exception:
                    raise
            else:
                raise
        except Exception:
            raise

    def chmod(self, mode, mtime=None, user=None, group=None,
              follow_symlinks=True):
        if follow_symlinks:
            os.chmod(self.url_str, mode)
        else:
            try:
                os.chmod(self.url_str, mode, follow_symlinks=False)
            except NotImplementedError:
                if not self.is_symlink():
                    raise
                # ignore, not changed for symlink
        if mtime is not None:
            self.utime(mtime, mtime, follow_symlinks=follow_symlinks)
        self.chown(user, group)

    def chown(self, user, group, follow_symlinks=False):
        if user is not None or group is not None:
            # shutil.chown does not have follow_symlinks option.
            uid = get_uid(user)
            gid = get_gid(group)
            os.chown(self.url_str, uid, gid, follow_symlinks=follow_symlinks)

    def utime(self, atime, mtime, follow_symlinks=True):
        os.utime(self.url_str, times=(atime, mtime),
                 follow_symlinks=follow_symlinks)

    def mkdir(self, mode=0o700, parents=False):
        if parents:
            os.makedirs(self.url_str, mode=mode, exist_ok=True)
        else:
            os.mkdir(self.url_str, mode)

    def remove_tree(self, remove_readonly=False):
        path = self.path
        if path == '/' or path == '':
            raise AssertionError
        if remove_readonly:
            def force_remove(func, target, _):
                if target != path:
                    parent = os.path.dirname(target)
                    os.chmod(parent, 0o700)
                os.chmod(target, 0o700)
                func(target)

            shutil.rmtree(path, onerror=force_remove)
        else:
            shutil.rmtree(path)

    def symlink(self, linkname):
        os.symlink(linkname, self.url_str)

    def hardlink(self, linkname):
        os.link(linkname, self.url_str)

    def exists(self):
        return os.path.exists(self.url_str)

    def is_directory(self):
        return os.path.isdir(self.url_str)

    def is_symlink(self):
        return os.path.islink(self.url_str)

    def is_empty(self):
        return len(os.listdir(self.url_str)) == 0

    def get_size(self):
        st = os.stat(self.url_str, follow_symlinks=False)
        logger.debug('get_size: %s, %d', self.url_str, st.st_size)
        return st.st_size

    @classmethod
    def _readlink(cls, path, is_symlink):
        if is_symlink:
            linkname = os.readlink(path)
        else:
            linkname = ''
        return linkname

    @classmethod
    def uid2name(cls, uid):
        try:
            pw = pwd.getpwuid(uid)
            return pw.pw_name
        except Exception:
            return str(uid)

    @classmethod
    def gid2name(cls, gid):
        try:
            gr = grp.getgrgid(gid)
            return gr.gr_name
        except Exception:
            return str(gid)

    @classmethod
    def _toFileType(cls, st):
        file_type = GfURLEntry.TYPE_OTHER
        if stat.S_ISREG(st.st_mode):
            file_type = GfURLEntry.TYPE_FILE
        elif stat.S_ISDIR(st.st_mode):
            file_type = GfURLEntry.TYPE_DIR
        elif stat.S_ISLNK(st.st_mode):
            file_type = GfURLEntry.TYPE_SYMLINK
        return file_type

    def _toGfURLEntry(self, entry, path_only, hardlink_warn):
        start = self
        if path_only:
            return GfURLEntry(start, entry.path, 0, None,
                              'root', 'root',
                              0, 0, '')
        st = entry.stat(follow_symlinks=False)
        file_type = self._toFileType(st)
        if file_type == GfURLEntry.TYPE_FILE:
            if hardlink_warn and st.st_nlink >= 2:
                logger.warning('hard link is not supported: '
                               'nlink=%d, inode=%d (Local): %s',
                               st.st_nlink, st.st_ino, entry.path)
        linkname = self._readlink(entry.path, entry.is_symlink())
        return GfURLEntry(start, entry.path, st.st_mode, file_type,
                          self.uid2name(st.st_uid),
                          self.gid2name(st.st_gid),
                          st.st_size, st.st_mtime, linkname)

    def _scandir(self, path, path_only, recursive, first, hardlink_warn):
        if first:
            st = os.stat(path, follow_symlinks=True)
            start = self
            yield GfURLEntry(start, path, st.st_mode, self._toFileType(st),
                             self.uid2name(st.st_uid),
                             self.gid2name(st.st_gid),
                             st.st_size, st.st_mtime, '')

        first = False
        for entry in os.scandir(path):
            yield self._toGfURLEntry(entry, path_only, hardlink_warn)
            if recursive and entry.is_dir(follow_symlinks=False):
                yield from self._scandir(entry.path, path_only, recursive,
                                         first, hardlink_warn)

    def listdir(self, path_only=False, recursive=False, first=False,
                hardlink_warn=True):
        yield from self._scandir(self.url_str, path_only, recursive, first,
                                 hardlink_warn)

    # NOTE: This is not expected behavior.
    # - This can copy a hard link,
    #   but a hard link cannot be extracted from gfexport (stream open)
    # - When the specified <member> for --create is a symlink,
    #   the entry will be archived as symlink.
    # def tar_add(self, tar, subpath, entry):
    #     if entry.path:
    #         path = os.path.join(self.url_str, entry.path)
    #     else:
    #         path = self.url_str
    #     tar.add(path, arcname=subpath, recursive=False)

    @contextmanager
    def readopen(self, textmode=False):
        if textmode:
            f = open(self.url_str, 'rt', encoding=get_encoding())
        else:
            f = open(self.url_str, 'rb')
        try:
            yield f
        finally:
            f.close()

    @contextmanager
    def writeopen(self, textmode=False, mode=0o600, mtime=None,
                  user=None, group=None):
        tmpmode = mode | 0o200  # necessary (Permission denied at ex.0o400)
        fd = os.open(path=self.url_str,
                     flags=(os.O_WRONLY | os.O_CREAT | os.O_EXCL),
                     mode=tmpmode,
                     )
        if textmode:
            f = open(fd, 'wt', encoding=get_encoding(), closefd=True)
        else:
            f = open(self.url_str, 'wb', closefd=True)
        try:
            yield f
        finally:
            f.close()
            if mode is not None:
                self.chmod(mode, mtime=mtime, user=user, group=group)
            else:
                if mtime is not None:
                    self.utime(mtime, mtime)
                self.chown(user, group)


class GfTarFile(tarfile.TarFile):
    COMPRESS_TYPE_NO = 'no'
    ATTR_PROC_LIST = '_gfptar_proc_list'  # [(proc, fileobj), ...]

    @classmethod
    def extract_open(cls, gfurl, bufsize, compress_prog=None):
        # use Stream (not seekable)
        if compress_prog is not None:
            openmode = 'r|'
        else:
            openmode = 'r|*'  # any type (gz, bz2, xz)
        if not gfurl.exists():
            raise FileNotFoundError(gfurl.url_str)
        proc_list = []
        if gfurl.is_gfarm():
            if compress_prog:
                gfexport_proc = gfurl.gfexport()
                decompress_proc = cls.decompress(compress_prog,
                                                 gfexport_proc.stdout)
                tar = cls.open(None, mode=openmode,
                               fileobj=decompress_proc.stdout,
                               bufsize=bufsize)
                proc_list.append(tuple([gfexport_proc,
                                        gfexport_proc.stdout]))
                proc_list.append(tuple([decompress_proc,
                                        decompress_proc.stdout]))
            else:
                gfexport_proc = gfurl.gfexport()
                tar = cls.open(None, mode=openmode,
                               fileobj=gfexport_proc.stdout,
                               bufsize=bufsize)
                proc_list.append(tuple([gfexport_proc,
                                        gfexport_proc.stdout]))
        else:
            if compress_prog:
                inf = open(gfurl.url_str, 'rb')
                decompress_proc = cls.decompress(compress_prog, inf)
                tar = cls.open(None, mode=openmode,
                               fileobj=decompress_proc.stdout,
                               bufsize=bufsize)
                proc_list.append(tuple([None, inf]))
                proc_list.append(tuple([decompress_proc,
                                        decompress_proc.stdout]))
            else:
                tar = GfTarFile.open(gfurl.url_str, mode=openmode,
                                     bufsize=bufsize)
        setattr(tar, cls.ATTR_PROC_LIST, proc_list)
        return tar

    @classmethod
    def create_open(cls, gfurl, compress_type, bufsize, compress_prog=None):
        # use Stream (not seekable)
        openmode = 'w|'
        if compress_prog is None \
           and compress_type != cls.COMPRESS_TYPE_NO:
            openmode = 'w|' + compress_type
        if gfurl.exists():
            raise FileExistsError(gfurl.url_str)
        proc_list = []
        if gfurl.is_gfarm():
            if compress_prog:
                gfreg_obj = gfurl.gfreg(mode=0o600)
                compress_proc = cls.compress(compress_prog,
                                             gfreg_obj.stdin)
                tar = cls.open(None, mode=openmode,
                               fileobj=compress_proc.stdin, bufsize=bufsize)
                proc_list.append(tuple([compress_proc, compress_proc.stdin]))
                proc_list.append(tuple([gfreg_obj.proc, gfreg_obj]))
            else:
                gfreg_obj = gfurl.gfreg(mode=0o600)
                tar = cls.open(None, mode=openmode,
                               fileobj=gfreg_obj.stdin,
                               bufsize=bufsize)
                proc_list.append(tuple([gfreg_obj.proc, gfreg_obj]))
        else:  # Local
            if compress_prog:
                outf = open(gfurl.url_str, 'wb')
                compress_proc = cls.compress(compress_prog, outf)
                tar = cls.open(None, mode=openmode,
                               fileobj=compress_proc.stdin, bufsize=bufsize)
                proc_list.append(tuple([compress_proc, compress_proc.stdin]))
                proc_list.append(tuple([None, outf]))
            else:
                tar = cls.open(gfurl.url_str, mode=openmode, bufsize=bufsize)
            gfurl.chmod(0o600)
        logger.debug('GfTarFile.create_open: %s', gfurl.url_str)
        setattr(tar, cls.ATTR_PROC_LIST, proc_list)
        return tar

    @classmethod
    def compress(cls, compress_prog, outf):
        args = [compress_prog]
        # binary mode
        return subprocess.Popen(
            args, shell=False, close_fds=True,
            stdin=subprocess.PIPE, stdout=outf,
            stderr=sys.stderr.buffer)

    @classmethod
    def decompress(cls, compress_prog, inf):
        args = [compress_prog, '-d']
        # binary mode
        return subprocess.Popen(
            args, shell=False, close_fds=True,
            stdin=inf, stdout=subprocess.PIPE,
            stderr=sys.stderr.buffer)

    # override
    def close(self):
        super().close()
        proc_list = getattr(self, self.ATTR_PROC_LIST, None)
        if proc_list:
            for proc_tuple in proc_list:
                proc, close_obj = proc_tuple
                if close_obj:
                    close_obj.close()
                if proc is not None:
                    logger.debug('close external process for tar: %s',
                                 str(proc.args))
                    ret = proc.wait()
                    if ret != 0:
                        raise Exception('{}: returncode={}'.format(
                            ' '.join(proc.args), ret))
                if close_obj:
                    post_func = getattr(close_obj, 'post', None)
                    if post_func:
                        close_obj.post()

    def add_entry(self, subpath, entry):
        entry.start_url.tar_add(self, subpath, entry)


class TestGfptar(unittest.TestCase):
    @staticmethod
    def suite():
        suite = unittest.TestSuite()
        suite.addTest(TestGfptar('test_unhumanize'))
        suite.addTest(TestGfptar('test_GfURL_use_gfarm_command_for_local'))
        return suite

    def test_unhumanize(self):
        self.assertEqual(unhumanize_number('1K'), 1024)
        self.assertEqual(unhumanize_number('2M'), 2097152)
        self.assertEqual(unhumanize_number('3G'), 3221225472)
        self.assertEqual(unhumanize_number('4T'), 4398046511104)
        self.assertEqual(unhumanize_number('5P'), 5629499534213120)
        self.assertEqual(unhumanize_number('6E'), 6917529027641081856)

    def test_GfURL_use_gfarm_command_for_local(self):
        url = GfURL.init('/tmp', use_gfarm_command=True)
        self.assertEqual(url.is_local(), True)
        self.assertEqual(url.path, '/tmp')


class GfptarCommand(Command):
    LIST_SUFFIX = '.lst'

    def __init__(self, name):
        self.init(name)
        set_encoding(self.opt['--encoding'])
        self.jobs = self.opt['--jobs']
        self.bufsize = self.opt['--bufsize']
        self.progress_enabled = self._progress_enabled()
        self.hardlink_warn = True

    def getDoc(self) -> str:
        return __doc__

    def getSchema(self) -> Schema:
        return _schema

    def _progress_enabled(self):
        return not self.debug and not self.verbose and not self.quiet

    def run(self):
        self.logger.debug(pformat(self.opt))
        try:
            outdir = self.opt['--create']
            if outdir:
                basedir = self.opt['--basedir']
                infiles = self.opt['<member>']
                self.create(outdir, basedir, infiles)
                return
            outdir = self.opt['--extract']
            if outdir:
                indir = self.opt['<indir>']
                members = self.opt['<member>']
                self.extract(outdir, indir, members)
                return
            indir = self.opt['--list']
            if indir:
                if self.verbose:
                    self.list_verbose(indir)
                else:
                    self.list_simple(indir)
                return
            if self.opt['--test']:
                self.test_main()
                return
        except Exception as e:
            if self.debug:
                raise
            else:
                if type(e) == Exception:  # custom errors
                    logger.error(str(e))
                else:
                    logger.error('%s: %s', e.__class__.__name__, str(e))
                sys.exit(1)

    def test_main(self):
        self.hardlink_warn = False
        self.uid = os.getuid()
        self.pid = os.getpid()
        out = execcmd(['gfwhoami'], textmode=True)
        self.gfarm_user = out.strip()
        self.test_unit()
        self.test_opt_pattern()
        self.test_specified_dir()

    def test_unit(self):
        verbosity = 2
        runner = unittest.TextTestRunner(verbosity=verbosity)
        result = runner.run(TestGfptar.suite())
        if len(result.errors) > 0 or len(result.failures) > 0:
            logger.error_exit(1, 'unittest error')
        print('unittest ... PASS')

    def test_opt_pattern(self):
        save_opt_size = self.opt['--size']
        save_opt_jobs = self.opt['--jobs']
        save_opt_type = self.opt['--type']
        save_opt_compress_prog = self.opt['--use-compress-program']

        # create tar per one entry
        self.opt['--size'] = 0
        pattern_jobs = [1, 4, 10]
        for jobs in pattern_jobs:
            self.opt['--jobs'] = jobs
            self.test_simple('jobs_' + str(jobs))
        self.opt['--jobs'] = save_opt_jobs

        # create one tar
        self.opt['--size'] = unhumanize_number('100M')
        pattern_type = [
            'gz',
            # 'bz2',
            # 'xz',
            'no']
        for t in pattern_type:
            self.opt['--type'] = t
            self.test_simple('type_' + t)
        self.opt['--type'] = save_opt_type

        pattern_compress_prog = {
            # 'gz': 'gzip',
            # 'bz2': 'bzip2',
            'xz': 'xz',
            # 'lzip': 'lzip',
            # 'lzop': 'lzop',
        }
        for t, prog in pattern_compress_prog.items():
            w = shutil.which(prog)
            if not w:
                logger.error('SKIPPED: No such command: %s', prog)
                continue
            self.opt['--type'] = t
            self.opt['--use-compress-program'] = prog
            self.test_simple('compress_prog_' + prog)
        # self.opt['--use-compress-program'] = save_opt_compress_prog

        self.opt['--size'] = save_opt_size
        self.opt['--jobs'] = save_opt_jobs
        self.opt['--type'] = save_opt_type
        self.opt['--use-compress-program'] = save_opt_compress_prog

    def test_simple(self, name):
        testname = f'gfptar-test-simple-{name}'
        d_name = f'{testname}-{self.gfarm_user}-{self.uid}-{self.pid}'
        workdir_local = os.path.join(self.opt['--test-workdir-local'], d_name)
        workdir_gfarm = os.path.join(self.opt['--test-workdir-gfarm'], d_name)
        workdir_local_url = GfURL.init(workdir_local)
        workdir_gfarm_url = GfURL.init(workdir_gfarm)
        workdir_local_url.mkdir()
        workdir_gfarm_url.mkdir()

        testsrc_name = 'test-src'
        srcdir_local = workdir_local_url.url_join(testsrc_name)
        srcdir_gfarm = workdir_gfarm_url.url_join(testsrc_name)
        self.test_prepare_srcdir(srcdir_local)
        self.test_prepare_srcdir(srcdir_gfarm)

        test1_name = 'test-1-create'
        test1_dir_gfarm = workdir_gfarm_url.url_join(test1_name)
        test2_name = 'test-2-extract'
        test2_dir_gfarm = workdir_gfarm_url.url_join(test2_name)
        test3_name = 'test-3-create'
        test3_dir_local = workdir_local_url.url_join(test3_name)
        test4_name = 'test-4-extract'
        test4_dir_local = workdir_local_url.url_join(test4_name)

        # Gfarm -> Gfarm(tar)
        self.create(test1_dir_gfarm, workdir_gfarm, [testsrc_name])
        # Gfarm(tar) -> Gfarm
        self.extract(test2_dir_gfarm, test1_dir_gfarm, [])
        # Gfarm -> Local(tar)
        self.create(test3_dir_local, test2_dir_gfarm, [testsrc_name])
        # Local(tar) -> Local
        self.extract(test4_dir_local, test3_dir_local, [])

        # --list
        self.list_simple(test1_dir_gfarm, quiet=True)
        self.list_simple(test3_dir_local, quiet=True)
        self.list_verbose(test1_dir_gfarm, quiet=True)
        self.list_verbose(test3_dir_local, quiet=True)

        # extract a member (SEE ALSO: test_prepare_srcdir)
        member = testsrc_name + '/dir1/ 2/ 2'
        all_members = [testsrc_name,
                       testsrc_name + '/dir1',
                       testsrc_name + '/dir1/ 2',
                       member,
                       ]
        test_member_g_name = 'test-gfptar-member-g'
        test_member_l_name = 'test-gfptar-member-l'
        test_member_g = workdir_gfarm_url.url_join(test_member_g_name)
        test_member_l = workdir_local_url.url_join(test_member_l_name)
        self.extract(test_member_g, test1_dir_gfarm, [member])
        self.extract(test_member_l, test3_dir_local, [member])
        g_member = GfURL.init(os.path.join(test_member_g, member))
        l_member = GfURL.init(os.path.join(test_member_l, member))
        if not g_member.compare_data(l_member):
            logger.error_exit(1, testname
                              + '(extract a member) ' +
                              '... FAIL (data mismatch)')
        for testdir in [test_member_g, test_member_l]:
            url = GfURL.init(testdir)
            for ent in url.listdir(recursive=True):
                p = ent.subpath(url)
                if p in all_members:
                    continue
                logger.error('unexpected member: %s', p)
                logger.error_exit(1, testname + '(extract a member) ' +
                                  '... FAIL (unexpected member found)')

        test5_name = 'test-5-create'
        test5_dir_gfarm = workdir_gfarm_url.url_join(test5_name)
        test6_name = 'test-6-extract'
        test6_dir_local = workdir_local_url.url_join(test6_name)

        # NOTE: check ignoring hardlinks for Local
        # Local -> Gfarm(tar)
        self.create(test5_dir_gfarm, workdir_local, [testsrc_name])
        # Gfarm(tar) -> Local
        self.extract(test6_dir_local, test5_dir_gfarm, [])

        if self.test_compare_local(test4_dir_local, test6_dir_local,
                                   same_owner=True):
            print(testname + ' ... PASS')
            workdir_local_url.remove_tree(remove_readonly=True)
            workdir_gfarm_url.remove_tree(remove_readonly=True)
        else:
            logger.error_exit(1, testname + ' ... FAIL')

    def test_prepare_srcdir(self, dir_url_str):
        F = 'file'
        D = 'directory'
        S = 'symlink'
        L = 'hardlink'
        longname = ('0123456789' * 30)[:255]
        tree = [
            (F, 'file1', 0o664, 1234567890, None),
            (D, 'dir1', 0o715, 2234567890, None),
            (D, 'dir1/readonly', 0o700, 3234567891, None),  # chmod 500 later
            (F, 'dir1/readonly/file2', 0o456, 4234567892, None),
            (D, 'dir1/ 2', 0o755, 5234567890, None),
            (F, 'dir1/ 2/ 2', 0o400, 6234567890, None),
            (S, 'dir1/ 2/ 2',
             0o777, 7234567890, ' 2'),
            (L, 'dir1/ 2/ 2',
             0o400, 8234567890, 'dir1/ 2/ 2'),
            (F, 'dir1/' + longname, 0o775, 9234567890, None),
            ]
        srcdir_url = GfURL.init(dir_url_str)
        srcdir_url.mkdir()
        for ent in tree:
            ftype = ent[0]
            path = ent[1]
            mode = ent[2]
            mtime = ent[3]
            linkname = ent[4]
            url = GfURL.init(srcdir_url.url_join(path))
            if ftype == F:
                with url.writeopen(textmode=True,
                                   mode=mode,
                                   mtime=mtime) as f:
                    f.write(path)
            elif ftype == D:
                url.mkdir()
            elif ftype == S:
                url.symlink(linkname)
                url.chmod(mode, mtime=mtime, follow_symlinks=False)
            elif ftype == L:
                url.hardlink(srcdir_url.url_join(linkname))
                url.chmod(mode, mtime=mtime)
        for ent in reversed(tree):
            ftype = ent[0]
            path = ent[1]
            mode = ent[2]
            mtime = ent[3]
            # linkname = ent[4]
            url = GfURL.init(srcdir_url.url_join(path))
            if ftype == D:
                url.chmod(mode, mtime=mtime)
        srcdir_url.chmod(mode=0o700, mtime=0)
        readonly_url = GfURL.init(srcdir_url.url_join('dir1/readonly'))
        readonly_url.chmod(mode=0o500)

    def test_specified_dir(self):
        basedir = self.opt['--basedir']
        if basedir is None:
            return
        infiles = self.opt['<member>']
        if len(infiles) == 0:
            return

        basedir_url = GfURL.init(basedir)

        testname = 'gfptar-test-specified-dir'
        d_name = f'{testname}-{self.gfarm_user}-{self.uid}-{self.pid}'
        workdir_local = os.path.join(self.opt['--test-workdir-local'], d_name)
        workdir_gfarm = os.path.join(self.opt['--test-workdir-gfarm'], d_name)
        workdir_local_url = GfURL.init(workdir_local)
        workdir_gfarm_url = GfURL.init(workdir_gfarm)
        workdir_local_url.mkdir()
        workdir_gfarm_url.mkdir()

        test9_name = 'test-9-create'
        test9_dir_gfarm = workdir_gfarm_url.url_join(test9_name)
        test10_name = 'test-10-extract'
        test10_dir_gfarm = workdir_gfarm_url.url_join(test10_name)
        test11_name = 'test-11-create'
        test11_dir_local = workdir_local_url.url_join(test11_name)
        test12_name = 'test-12-extract'
        test12_dir_local = workdir_local_url.url_join(test12_name)

        # basedir -> Gfarm(tar)
        self.create(test9_dir_gfarm, basedir, infiles)
        # Gfarm(tar) -> Gfarm
        self.extract(test10_dir_gfarm, test9_dir_gfarm, [])
        # Gfarm -> Local(tar)
        self.create(test11_dir_local, test10_dir_gfarm, infiles)
        # Local(tar) -> Local
        self.extract(test12_dir_local, test11_dir_local, [])

        result = True
        for infile in infiles:
            src = basedir_url.url_join(infile)
            result_dir = os.path.join(test12_dir_local, infile)
            # owner on local and Gfarm may be different
            same_owner = not basedir_url.is_gfarm()
            if not self.test_compare(result_dir, src, data=True,
                                     same_owner=same_owner):
                result = False
                break
        if result:
            print(testname + ' ... PASS')
            workdir_local_url.remove_tree(remove_readonly=True)
            workdir_gfarm_url.remove_tree(remove_readonly=True)
        else:
            logger.error_exit(1, testname + ' ... FAIL')

    def test_compare_local(self, dir1, dir2, data=False, same_owner=False):
        out, err, ret = execcmd_raw(['diff', '-r', dir1, dir2])
        if ret != 0:
            return False
        logger.debug('diff -r (data check): PASS')
        return self.test_compare(dir1, dir2, data=data, same_owner=same_owner)

    def test_compare(self, dir1, dir2, data=True, same_owner=False):
        # compare mtime and mode of dir1/files with dir2/files
        d1url = GfURL.init(dir1)
        d2url = GfURL.init(dir2)
        return d1url.compare_entries(d2url, data=True, same_owner=same_owner)

    def MT_enabled(self):
        return self.jobs >= 2

    # lock required
    def info(self, fmt, *args):
        if self.verbose:
            print(fmt.format(*args))

    def create(self, outdir, basedir, infiles):
        self.outdir = outdir
        self.outdir_url = GfURL.init(outdir)
        self.outdir_url.create_new_dir()
        self.basedir_url = GfURL.init(basedir)
        self.size = self.opt['--size']
        self.ratio = self.opt['--ratio']
        self.compress_type = self.opt['--type']
        self.compress_prog = self.opt['--use-compress-program']
        self.disable_gfarm_command = self.opt['--disable-gfarm-command']
        self.use_gfarm_command = not self.disable_gfarm_command
        self.use_tqdm = self.opt['--use-tqdm']
        if self.compress_type == GfTarFile.COMPRESS_TYPE_NO:
            self.split_size = self.size
            self.suffix = '.tar'
        else:
            self.split_size = self.size * 100 / self.ratio
            self.suffix = '.tar.' + self.compress_type

        def entry_key(entry):
            return entry.path

        self.filelistlist = []
        self.total_size = 0
        self.total_num = 0
        for infile in infiles:
            infile_url = GfURL.init(infile)
            if not infile_url.is_local():
                raise Exception('a relative path is required '
                                'instead of a URL: ' + infile)
            infile = infile_url.path  # normalize and ignore scheme
            infile = infile.lstrip('/')  # relative path only
            filelist = []
            url_str = os.path.join(self.basedir_url.url_str, infile)
            gfurl = GfURL.init(url_str)
            logger.debug('listdir: %s', gfurl.url_str)
            for entry in gfurl.listdir(recursive=True, first=True,
                                       hardlink_warn=self.hardlink_warn):
                logger.debug('listdir: entry.path=%s', entry.path)
                filelist.append(entry)
                self.total_size += entry.size
                self.total_num += 1
            filelist.sort(key=entry_key, reverse=False)
            self.filelistlist.extend(self.schedule(filelist))

        self.tqdm = None
        self.myprogress = False
        if self.progress_enabled:
            if self.use_tqdm and have_tqdm:
                term_size = shutil.get_terminal_size()
                # bar_format = '{l_bar}{r_bar}'
                bar_format = '{percentage:3.0f}% {n_fmt}/{total_fmt}' \
                    + ' [{elapsed}<{remaining}, {rate_fmt}]'
                self.tqdm = tqdm(total=self.total_size, unit_scale=True,
                                 unit='B', dynamic_ncols=False,
                                 bar_format=bar_format,
                                 ncols=int(term_size.columns*3/4))
            else:
                self.myprogress = True

        self.serial = 0
        self.cancelled = False
        self.archive_size = 0
        self.stored_size = 0
        self.stored_num = 0
        self.start_time = time.time()
        self.next_time = self.start_time + 1
        if self.MT_enabled():
            self.create_tars_MT()
        else:
            self.create_tars()
        if self.myprogress:
            self.progress_for_create(time.time())
            sys.stderr.write('\n')
        if self.tqdm:
            self.tqdm.close()
        if not self.quiet and self.stored_size > 0:
            print('compression ratio result: %.2f %% (%d/%d)' %
                  (100 * self.archive_size / self.stored_size,
                   self.archive_size, self.stored_size))
        del self.filelistlist

    def create_tars(self):
        self.lock_init(False)
        for filelist in self.filelistlist:
            self.serial += 1
            self.create_a_tar(self.serial, filelist)

    def create_tars_MT(self):
        self.lock_init(True)
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.jobs) as executor:
            futures = {}
            for filelist in self.filelistlist:
                self.serial += 1
                t = executor.submit(self.create_a_tar, self.serial, filelist)
                futures[t] = self.serial
            for t in concurrent.futures.as_completed(futures, timeout=None):
                exc = t.exception()
                if exc:
                    logger.error('%s: serial=%04d', repr(exc), futures[t])
                    tb = traceback.TracebackException.from_exception(exc)
                    logger.debug(''.join(tb.format()))
                    if not self.is_cancelled():
                        self.cancel()
                        for t2 in futures:
                            logger.error("cancel: serial=%04d", futures[t2])
                            t2.cancel()
                    raise exc

    def schedule(self, filelist):
        filelistlist = []
        newlist = []
        total = 0
        for entry in filelist:
            total += entry.size
            if total >= self.split_size:
                if len(newlist) > 0:
                    filelistlist.append(newlist)
                newlist = []
                total = entry.size
            newlist.append(entry)
        if len(newlist) > 0:
            filelistlist.append(newlist)
        return filelistlist

    def lock_init(self, enable):
        if enable:
            self._lock = threading.Lock()
        else:
            self._lock = None

    @contextmanager
    def lock(self):
        if self._lock:
            self._lock.acquire()
        try:
            yield
        finally:
            if self._lock:
                self._lock.release()

    def cancel(self):
        with self.lock():
            self.cancelled = True

    def is_cancelled(self):
        with self.lock():
            val = self.cancelled
        return val

    def create_a_tar(self, serial, filelist):
        logger.debug('create_a_tar: start: %04d', serial)
        if self.is_cancelled():
            logger.debug('cancelled (1): serial=%04d', serial)
            return
        first = None
        last = None
        for entry in filelist:
            if entry.is_file():
                first = entry
                break
        if first is None:
            first = filelist[0]
        for entry in reversed(filelist):
            if entry.is_file():
                last = entry
        if last is None or first == last:
            firstpath = first.subpath(self.basedir_url)
            outname = '%s%s' % (firstpath, self.suffix)
        else:
            firstpath = first.subpath(self.basedir_url)
            lastpath = last.subpath(self.basedir_url)
            outname = '%s..%s%s' % (firstpath, lastpath, self.suffix)

        serial_str = '%04d_' % serial
        outname_max = GfURL.MAXNAMLEN - len(serial_str) - len(self.LIST_SUFFIX)
        if len(outname) > outname_max:
            # use last half of name
            outname = outname[-outname_max:]
        # ex.: home/user1/dir -> home_user1_dir
        outname = serial_str + outname.replace('/', '_')
        outurl = GfURL.init(self.outdir_url.url_join(outname),
                            use_gfarm_command=self.use_gfarm_command)
        self.create_a_members_list(outurl, filelist)
        tar = GfTarFile.create_open(outurl, self.compress_type, self.bufsize,
                                    compress_prog=self.compress_prog)
        for entry in filelist:
            if self.is_cancelled():
                logger.debug('cancelled (2): serial=%04d', serial)
                break
            subpath = entry.subpath(self.basedir_url)
            tar.add_entry(subpath, entry)

            with self.lock():
                self.info('stored: {}', subpath)
                self.stored_size += entry.size
                if self.myprogress:
                    self.stored_num += 1
                    now = time.time()
                    if now >= self.next_time:
                        self.next_time += 1
                        self.progress_for_create(now)
                elif self.tqdm:
                    if entry.is_file():
                        self.tqdm.update(entry.size)

        tar.close()
        tar_size = outurl.get_size()
        with self.lock():
            self.info('created(.tar): {}', outurl.url_str)
            self.archive_size += tar_size

    def create_a_members_list(self, url, filelist):
        outurl = GfURL.init(url.url_str + self.LIST_SUFFIX)
        with outurl.writeopen(textmode=True) as f:
            for entry in filelist:
                # ex. "D /path/to/dir"
                if entry.is_file():
                    f.write('F ')
                elif entry.is_directory():
                    f.write('D ')
                elif entry.is_symlink():
                    f.write('S ')
                else:  # unknown
                    f.write('? ')
                subpath = entry.subpath(self.basedir_url)
                f.write(subpath)
                f.write('\n')
        self.info('created(.lst): {}', outurl.url_str)

    def error_not_a_gfptar_directory(self, url_str):
        raise Exception('Not a gfptar-archived directory: ' + url_str)

    def extract(self, outdir, indir, members):
        self.outdir = outdir
        self.outdir_url = GfURL.init(outdir)
        self.outdir_url.create_new_dir()
        self.indir = indir
        member_set = set(members)
        self.compress_prog = self.opt['--use-compress-program']
        self.same_owner = self.opt['--same-owner']
        self.disable_gfarm_command = self.opt['--disable-gfarm-command']
        self.use_gfarm_command = not self.disable_gfarm_command

        indir_url = GfURL.init(self.indir)
        if not indir_url.exists():
            raise FileNotFoundError(indir_url.url_str)
        if not indir_url.is_directory():
            self.error_not_a_gfptar_directory(indir_url.url_str)

        search_target = len(member_set) > 0
        archive_dict = {}  # member -> (file_type, tar filename)
        target_set_all = set()  # all  tar files
        directory_list = []
        self.total_num = 0
        for ent in indir_url.listdir(recursive=False):
            if ent.path.endswith(self.LIST_SUFFIX):  # ignored
                continue
            subpath = ent.subpath(indir_url)
            arch_url_str = indir_url.url_join(subpath)
            target_set_all.add(arch_url_str)
            list_url = GfURL.init(arch_url_str + self.LIST_SUFFIX)
            if not list_url.exists():
                self.error_not_a_gfptar_directory(indir_url.url_str)
            with list_url.readopen(textmode=True) as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    self.total_num += 1
                    line = line.rstrip()
                    # ex. "D /path/to/dir"
                    file_type = line[:1]
                    path = line[2:].lstrip('/')
                    if search_target:
                        archive_dict[path] = (file_type, arch_url_str)
                    else:
                        if file_type == 'D':
                            directory_list.append(path)

        if search_target:
            # selected tar files
            target_set = set()
            directory_set = set()
            for member in member_set:
                type_arch = archive_dict.get(member, None)
                if type_arch is None:
                    raise Exception('Not found in archive: ' + member)
                file_type, target_arch = type_arch
                target_set.add(target_arch)
                if file_type == 'D':
                    directory_set.add(member)

            # add parent directories to update attributes (mode,mtime)
            new_member_set = set()
            for member in member_set:
                new_member_set.add(member)
                url = GfURL.init(member, local=True)  # relative path
                for parent_url in url.parent_list:
                    path = parent_url.path
                    if path != '.' and path != '/':
                        type_arch = archive_dict.get(path, None)
                        if type_arch is not None:  # found
                            file_type, target_arch = type_arch
                            target_set.add(target_arch)
                            directory_set.add(path)
                            new_member_set.add(path)
            del member_set
            member_set = new_member_set  # replace
            self.total_num = len(member_set)  # re-set
            del archive_dict
            directory_list = list(directory_set)
            del directory_set
        else:
            target_set = target_set_all

        target_list = list(target_set)
        del target_set
        target_list.sort()

        # self.extract_directories(directory_list)
        self.extract_directories_fast(directory_list)

        self.cancelled = False
        self.extracted_num = 0
        self.extracted_size = 0
        self.start_time = time.time()
        self.next_time = self.start_time + 1
        self.dirstat_dict = {}
        if self.MT_enabled():
            self.extract_from_archives_MT(target_list, member_set)
        else:
            self.extract_from_archives(target_list, member_set)
        if self.progress_enabled:
            self.progress_for_extract(time.time())
            sys.stderr.write('\n')

        self.update_stat_for_directories(directory_list, member_set)

    def extract_directories(self, directory_list):
        # NOTE: slow on Gfarm
        directory_list.sort()
        for d in directory_list:
            url_str = self.outdir_url.url_join(d)
            dir_url = GfURL.init(url_str)
            try:
                dir_url.mkdir()
            except Exception:
                dir_url.makedirs()
            self.info('prepare_dir: {}', dir_url.url_str)

    def extract_directories_fast(self, directory_list):
        # faster implementation for gfmkdir
        directory_list.sort(reverse=True)
        created = set()
        for d in directory_list:
            url_str = self.outdir_url.url_join(d)
            dir_url = GfURL.init(url_str)
            # url.path is normalized
            if dir_url.path in created:
                logger.debug('skip (already created): %s', url_str)
                continue
            parent_url = dir_url.parent
            if parent_url.path in created:
                dir_url.mkdir()
                created.add(dir_url.path)
                self.info('prepare_dir: {}', dir_url.path)
            else:  # no parent
                dir_url.makedirs()
                created.add(dir_url.path)
                self.info('prepare_dir: {}', dir_url.path)
                created.add(parent_url.path)
                self.info('prepare_dir: {}', parent_url.path)
                for p in parent_url.parent_list:
                    path = p.path
                    if path == '.' or path == '/':
                        continue
                    created.add(path)
                    self.info('prepare_dir: {}', path)
        del created

    def update_stat_for_directories(self, directory_list, member_set):
        # process from leaves
        directory_list.sort(reverse=True)
        members_num = len(member_set)
        for d in directory_list:
            if members_num > 0 and d not in member_set:
                continue
            tarinfo = self.dirstat_dict.get(d)
            if tarinfo is None:
                logger.warning('No information of the directory: %s', d)
                continue
            url_str = self.outdir_url.url_join(d)
            dir_url = GfURL.init(url_str)
            if self.same_owner:
                dir_url.chmod(tarinfo.mode, mtime=tarinfo.mtime,
                              user=tarinfo.uname, group=tarinfo.gname)
            else:
                dir_url.chmod(tarinfo.mode, mtime=tarinfo.mtime)
            logger.debug("update_stat: %s, %s", d, oct(tarinfo.mode))
            self.info('update_stat: {}', d)

    def extract_from_archives(self, target_list, member_set):
        self.lock_init(False)
        for target in target_list:
            logger.debug('target_set: %s', target)
            self.extract_from_a_tar(target, member_set)

    def extract_from_archives_MT(self, target_list, member_set):
        self.lock_init(True)
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.jobs) as executor:
            futures = {}
            for target in target_list:
                logger.debug('target_set: %s', target)
                t = executor.submit(self.extract_from_a_tar,
                                    target, member_set)
                futures[t] = target

            for t in concurrent.futures.as_completed(futures, timeout=None):
                exc = t.exception()
                if exc:
                    logger.error('%s: %s', repr(exc), futures[t])
                    tb = traceback.TracebackException.from_exception(exc)
                    logger.debug(''.join(tb.format()))
                    if not self.is_cancelled():
                        self.cancel()
                        for t2 in futures:
                            logger.error("cancel: name=%s", futures[t2])
                            t2.cancel()
                    raise exc

    def extract_from_a_tar(self, target, member_set):
        if self.is_cancelled():
            logger.debug('cancelled (1): name=%s', target)
            return
        arch_url = GfURL.init(target,
                              use_gfarm_command=self.use_gfarm_command)
        tar = GfTarFile.extract_open(arch_url, self.bufsize,
                                     compress_prog=self.compress_prog)
        members_num = len(member_set)
        while True:
            if self.is_cancelled():
                logger.debug('cancelled (2): name=%s', target)
                return
            tarinfo = tar.next()
            if tarinfo is None:
                break
            if members_num > 0:
                if tarinfo.name not in member_set:
                    continue  # not a target
            # members_num == 0 -> extract all

            outfile = tarinfo.name.lstrip('/')  # relative path only
            url_str = self.outdir_url.url_join(outfile)
            outurl = GfURL.init(url_str)

            if tarinfo.isfile():
                user = None
                group = None
                if self.same_owner:
                    user = tarinfo.uname
                    group = tarinfo.gname
                inf = tar.extractfile(tarinfo)  # io.BufferedReader
                size = outurl.copy_from(inf, self.bufsize,
                                        mode=tarinfo.mode,
                                        mtime=tarinfo.mtime,
                                        user=user, group=group)
                inf.close()
                logger.debug('extract,file: %s, %d', tarinfo.name, size)
                with self.lock():
                    self.info('extracted(F): {}', tarinfo.name)
            elif tarinfo.isdir():
                # NOTE: already created
                logger.debug('extract,dir: %s', tarinfo.name)
                with self.lock():
                    self.dirstat_dict[outfile] = tarinfo
            elif tarinfo.issym():
                logger.debug('extract,link: %s, %s', tarinfo.name,
                             tarinfo.linkname)
                outurl.symlink(tarinfo.linkname)
                if self.same_owner:
                    outurl.chmod(tarinfo.mode, mtime=tarinfo.mtime,
                                 user=tarinfo.uname, group=tarinfo.gname,
                                 follow_symlinks=False)
                else:
                    outurl.chmod(tarinfo.mode, mtime=tarinfo.mtime,
                                 follow_symlinks=False)
                with self.lock():
                    self.info('extracted(S): {}', tarinfo.name)
            else:
                logger.warning('unsupported type: %s: %s',
                               tarinfo.name, tarinfo.type)
            if self.progress_enabled:
                with self.lock():
                    self.extracted_num += 1
                    if tarinfo.isfile():
                        self.extracted_size += tarinfo.size
                    now = time.time()
                    if now >= self.next_time:
                        self.next_time += 1
                        self.progress_for_extract(now)

        tar.close()
        with self.lock():
            self.info('extracted(done): {}', arch_url.url_str)

    # lock required
    def progress_for_create(self, now):
        sec = now - self.start_time
        if self.total_size > 0:
            percent = self.stored_size * 100 / self.total_size
        else:
            percent = 0
        if sec > 0:
            bytes_per_sec = self.stored_size / sec
        else:
            bytes_per_sec = 0
        sys.stderr.write(f'\rcreated: {percent:.0f}%, '
                         f'num={self.stored_num}/{self.total_num}, '
                         f'size={self.stored_size}, '
                         f'sec={sec:.0f}, '
                         f'B/s={bytes_per_sec:.0f}  ')

    # lock required
    def progress_for_extract(self, now):
        sec = now - self.start_time
        if self.total_num > 0:
            percent = self.extracted_num * 100 / self.total_num
        else:
            percent = 0
        if sec > 0:
            bytes_per_sec = self.extracted_size / sec
        else:
            bytes_per_sec = 0
        sys.stderr.write(f'\rextracted: {percent:.0f}%, '
                         f'num={self.extracted_num}/{self.total_num}, '
                         f'size={self.extracted_size}, '
                         f'sec={sec:.0f}, '
                         f'B/s={bytes_per_sec:.0f}  ')

    def list_simple(self, indir, quiet=False):
        indir_url = GfURL.init(indir)
        filelistlist = []
        for ent in indir_url.listdir(recursive=False):
            if not ent.path.endswith(self.LIST_SUFFIX):
                continue
            filelistlist.append(ent.path)
        filelistlist.sort()
        for filelist in filelistlist:
            list_url = GfURL.init(filelist)
            with list_url.readopen(textmode=True) as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    if not quiet:
                        print(line.rstrip())

    def list_verbose(self, indir, quiet=False):
        self.compress_prog = self.opt['--use-compress-program']
        indir_url = GfURL.init(indir)
        archlist = []
        for ent in indir_url.listdir(recursive=False):
            if ent.path.endswith(self.LIST_SUFFIX):
                continue
            archlist.append(ent.path)
        archlist.sort()
        for path in archlist:
            arch_url = GfURL.init(path)
            tar = GfTarFile.extract_open(arch_url, self.bufsize,
                                         compress_prog=self.compress_prog)
            while True:
                t = tar.next()
                if t is None:
                    break
                name = t.name
                if t.isdir():
                    name = name + '/'
                info = (f'{t.mode:4o} {t.uname:>10}/{t.gname:<10}'
                        f' {t.size:9d} {t.mtime} {name}')
                if not quiet:
                    print(info)


progname = os.path.basename(__file__)


__doc__ = """
Usage:
  {f} [options] -c <outdir> [-C <basedir>] [--] <member>...
  {f} [options] -x <outdir> [--] <indir> [<member>...]
  {f} [options] -t <indir>
  {f} [options] --test
  {f} [options] --test -C <basedir> <member>...
  {f} -h | --help

Options:
  -t, --list=DIR            list mode,
                            list the members of <indir>
  -x, --extract=DIR         extract mode,
                            extract all members or specified <member>s
                            from <indir> to <outdir>
  -c, --create=DIR          create mode,
                            create tar files in <outdir> from <member>s
  -C, --basedir=DIR         base directory for <member>s  [default: .]
  -j, --jobs=NUM            the number of jobs to copy per tar file in parallel
                            [default: 4]
  -s, --size=BYTES          assumed bytes per output file [default: 200M]
  -T, --type=TYPE           compress type (gz,bz2,xz,no) [default: gz]
  -r, --ratio=RATIO         assumed compression ratio (%) [default: 50]
  -I, --use-compress-program=COMMAND
                            filter data through COMMAND,
                            the command must accept -d option for decompression
  --same-owner              extract files with the same ownership
                            (for euid=0 on local, or gfarmroot on Gfarm)
  --disable-gfarm-command   disable the use of gfreg and gfexport
                            for tar files on gfarm2fs
  --use-tqdm                use tqdm to show progress (for --create)
  --encoding=CODEC          codec for filename encoding
             (https://docs.python.org/3/library/codecs.html#standard-encodings)
                            [default: utf-8]
  --bufsize=BYTES           buffer size to copy [default: 1M]
  --test                    test mode (-q option is recommended)
  --test-workdir-local=DIR  local directory for test [default: /tmp]
  --test-workdir-gfarm=DIR  Gfarm directory for test [default: gfarm:/tmp]
  -q, --quiet               quiet messages
  -v, --verbose             verbose output
  -d, --debug               debug mode
  -?, -h, --help            show this help and exit

Example of --create (Gfarm to Gfarm):
  Command line:
    gfptar -c gfarm:/home/user1/out -C gfarm:/home/user1 ./dir
  Input files:
    gfarm:/home/user1/dir/test0000.data
    ...
    gfarm:/home/user1/dir/test9999.data
  Output files:
    gfarm:/home/user1/out/0001_dir_test0000.data..dir_test0999.data.tar.gz
    gfarm:/home/user1/out/0001_dir_test0000.data..dir_test0999.data.tar.gz.lst
    ...
    gfarm:/home/user1/out/0010_dir_test9000.data..dir_test9999.data.tar.gz
    gfarm:/home/user1/out/0010_dir_test9000.data..di1_test9999.data.tar.gz.lst
  Contents of list file (*.lst):
    F dir/test0000.data
    ...
    F dir/test0999.data

Example of --extract (Gfarm to Gfarm):
  Command line:
    gfptar -x gfarm:/home/user1/out2 gfarm:/home/user1/out
  Output files:
    gfarm:/home/user1/out2/dir/test0000.data
    ...
    gfarm:/home/user1/out2/dir/test9999.data

Limitations:
  - Hard links are not preserved.
  - File names cannot include newline characters.
  - Subsecond (less than a second) for mtime is not preserved.
""".format(f=progname)


_schema = Schema({
    '--list': Or(str, None),
    '--extract': Or(str, None),
    '--create': Or(str, None),
    '--basedir': Or(str, None),
    '--encoding': str,
    '--size': Use(unhumanize_number),
    '--bufsize': Use(unhumanize_number),
    '--type': str,
    '--ratio': Use(int),
    '--jobs': Use(int),
    '--use-compress-program': Or(str, None),
    '--disable-gfarm-command': bool,
    '--same-owner': bool,
    '--use-tqdm': bool,
    '--test': bool,
    '--test-workdir-local': Or(str, None),
    '--test-workdir-gfarm': Or(str, None),
    '--quiet': bool,
    '--verbose': bool,
    '--debug': bool,
    '--help': bool,
    '--': bool,
    '<indir>': Or(str, None),
    '<member>': [str],
})


if __name__ == '__main__':
    gfptar = GfptarCommand(progname)
    gfptar.run()

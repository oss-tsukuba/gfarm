#!/usr/bin/env python3

# Requirements:
# --- for Debian,Ubuntu series ---
# apt-get install python3 python3-docopt python3-schema
# (optional) python3-tqdm
#
# --- for RHEL,CentOS series ---
# yum install     epel-release
# yum install     python3 python3-docopt python3-schema
# (optional) python3-tqdm
#
# --- Or, install to ~/.local just for user's environment ---
# (required: python3-pip)
# pip3 install --user docopt schema tqdm

# Coding style check:
# flake8 ./gfptar

import os
import errno
import stat
import sys
import pwd
import grp
import time
import re
import abc
import tarfile
from pprint import pformat
import logging
import subprocess
import concurrent.futures
import threading
from typing import NoReturn
from urllib.parse import urlparse
import shutil
from contextlib import contextmanager
import traceback
import unittest
import hashlib
import signal

from docopt import docopt
from schema import Schema, Use, Or

try:
    from tqdm import tqdm
    have_tqdm = True
except Exception:
    have_tqdm = False


# library
def unhumanize_number(numstr):
    strlen = len(numstr)
    if strlen == 1:
        return int(numstr)
    n = int(numstr[:(strlen-1)])
    si_prefix = numstr[(strlen-1):]
    prefixes = {'K': 1,
                'M': 2,
                'G': 3,
                'T': 4,
                'P': 5,
                'E': 6,
                }
    power = prefixes.get(si_prefix.upper())
    if power is None:
        return int(numstr)
    return n * (1024 ** power)


class GfLogger(logging.getLoggerClass()):
    def __init__(self):
        super().__init__()
        self.myinit()

    def myinit(self):
        if getattr(self, 'lock', None) is None:
            self.lock = threading.Lock()

    # REFERENCE: logging/__init__.py: class Logger()
    def debug(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.DEBUG):
            with self.lock:
                self._log(logging.DEBUG, msg, args, **kwargs)

    def info(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.INFO):
            with self.lock:
                self._log(logging.INFO, msg, args, **kwargs)

    def warning(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.WARNING):
            with self.lock:
                self._log(logging.WARNING, msg, args, **kwargs)

    def error(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.ERROR):
            with self.lock:
                self._log(logging.ERROR, msg, args, **kwargs)

    def error_exit(self, exit_code, msg, *args, **kwargs):
        if self.isEnabledFor(logging.ERROR):
            with self.lock:
                self._log(logging.ERROR, msg, args, **kwargs)
        sys.exit(exit_code)

    def fatal(self, msg, *args, **kwargs):
        if self.isEnabledFor(logging.ERROR):
            with self.lock:
                self._log(logging.ERROR, msg, args, **kwargs)
            if 'exit_code' in kwargs:
                raise Exception('exit_code={}'.format(kwargs['exit_code']))
            else:
                raise Exception('fatal exit')


logger = None


def logger_init(name, loglevel=logging.WARNING, debug=False):
    global logger

    if logger is not None:
        return logger

    logger = logging.getLogger()  # RootLogger
    logger.__class__ = GfLogger
    logger.myinit()
    logger.setLevel(loglevel)
    strm = logging.StreamHandler()  # stderr
    if debug:
        fmt = '%(filename)s:%(levelname)s:L%(lineno)d:' + \
            '%(asctime)s: %(message)s'
        fmt = fmt.format(name)
    else:
        fmt = '{}:%(levelname)s: %(message)s'.format(name)
    formatter_strm = logging.Formatter(fmt=fmt, datefmt='%Y%m%d%H%M%S')
    strm.setFormatter(formatter_strm)
    strm.setLevel(loglevel)
    logger.addHandler(strm)
    return logger


_encoding = sys.getfilesystemencoding()


def get_encoding():
    return _encoding


def set_encoding(enc):
    global _encoding
    if _encoding != enc:
        _encoding = enc


def execcmd_raw(args, stdin=subprocess.DEVNULL, stderr=subprocess.PIPE,
                timeout=None, textmode=False):
    if textmode:
        encoding = get_encoding()
    else:
        encoding = None
    proc = subprocess.Popen(
        args, shell=False, encoding=encoding, close_fds=True,
        stdin=stdin, stdout=subprocess.PIPE, stderr=stderr)
    try:
        out, err = proc.communicate(timeout=timeout)
    except subprocess.TimeoutExpired:
        proc.kill()
        out, err = proc.communicate()
    ret = proc.wait()
    return out, err, ret


use_stderr = True
DEFAULT_STDERR = sys.stderr.buffer


def shutup_stderr():
    global use_stderr
    global DEFAULT_STDERR
    use_stderr = False
    DEFAULT_STDERR = subprocess.DEVNULL


def execcmd(args, stdin=subprocess.DEVNULL, stderr=subprocess.PIPE,
            timeout=None, textmode=False):
    out, err, ret = execcmd_raw(args, stdin=stdin, stderr=stderr,
                                timeout=timeout, textmode=textmode)
    if ret != 0:
        try:
            err = err.decode().rstrip()
        except Exception:
            pass
        msg = '{}: {}'.format(str(args), err)
        if err and use_stderr:
            logger.debug(msg)
        raise Exception(msg)
    return out


# text mode
def execcmd_readline(args, stdin=subprocess.DEVNULL, stderr=None):
    if stderr is None:  # for shutup_stderr
        stderr = DEFAULT_STDERR
    proc = subprocess.Popen(
        args, shell=False, encoding=get_encoding(), close_fds=True,
        stdin=stdin, stdout=subprocess.PIPE, stderr=stderr)
    while True:
        line = proc.stdout.readline()
        if line:
            line = line.rstrip('\r\n')
            yield line
        elif proc.poll() is not None:
            break
    ret = proc.wait()
    if ret != 0:
        raise Exception('{}: returncode={}'.format(' '.join(args), ret))


def gfwhoami():
    return execcmd(['gfwhoami'], textmode=True).strip()


def am_I_gfarmroot():
    out = execcmd(['gfgroup', '-l', 'gfarmroot'], textmode=True)
    patt = r'gfarmroot: {}( |$)'.format(gfwhoami())
    if re.match(patt, out.strip()):
        return True
    return False


class Command(metaclass=abc.ABCMeta):
    def init(self, name) -> NoReturn:
        self._docopt = docopt(self.getDoc())
        self.opt = self.getSchema().validate(self._docopt)
        self.debug = self.opt['--debug']
        self.verbose = self.opt['--verbose']
        self.quiet = self.opt['--quiet']

        loglevel = logging.WARNING
        if self.debug:
            loglevel = logging.DEBUG
        elif self.verbose:
            loglevel = logging.INFO
        elif self.quiet:
            loglevel = logging.ERROR
        self.loglevel = loglevel

        # use stderr
        mylogger = logger_init(name, loglevel=loglevel, debug=self.debug)
        self.logger = mylogger

        logger.debug('USE_GFMKDIR_PLUS: %s', USE_GFMKDIR_PLUS)
        logger.debug('USE_GFCHMOD_PLUS: %s', USE_GFCHMOD_PLUS)
        logger.debug('USE_GFREG_PLUS: %s', USE_GFREG_PLUS)

    @abc.abstractmethod
    def getDoc(self) -> str:
        raise NotImplementedError

    @abc.abstractmethod
    def getSchema(self) -> Schema:
        raise NotImplementedError

    @abc.abstractmethod
    def run(self, option) -> NoReturn:
        raise NotImplementedError


class GfURLEntry():
    TYPE_FILE = 'FILE'
    TYPE_DIR = 'DIR'
    TYPE_SYMLINK = 'SYM'
    TYPE_OTHER = 'OTHER'

    def __init__(self, start_url, path, mode, file_type, uname, gname,
                 size, mtime, linkname):
        self.start_url = start_url  # GfURL
        self.path = path
        self.mode = mode
        self.file_type = file_type
        self.uname = uname
        self.gname = gname
        self.size = size
        self.mtime = mtime
        self.linkname = linkname

        if not self.is_file():
            self.size = 0

    def __str__(self):
        return f'{self.path}, {self.mode:o}, {self.file_type}'

    def __repr__(self):
        return str(self)

    def subpath(self, baseurl):
        return baseurl.subpath(self.path)

    def is_file(self):
        return self.file_type == self.TYPE_FILE

    def is_directory(self):
        return self.file_type == self.TYPE_DIR

    def is_symlink(self):
        return self.file_type == self.TYPE_SYMLINK

    def toTarinfo(self, path):
        tarinfo = tarfile.TarInfo(path)
        if self.is_file():
            tarinfo.type = tarfile.REGTYPE
        elif self.is_directory():
            tarinfo.type = tarfile.DIRTYPE
        elif self.is_symlink():
            tarinfo.type = tarfile.SYMTYPE
        else:
            logger.warning('unsupported type: %s, %s', path, self.file_type)
            return None
        tarinfo.mode = self.mode
        tarinfo.mtime = self.mtime
        tarinfo.size = self.size
        tarinfo.linkname = self.linkname
        tarinfo.uname = self.uname
        tarinfo.gname = self.gname
        return tarinfo

    def compare(self, ent2, data=False, same_owner=False, bufsize=1048576):
        ent1 = self

        def cmpval(a, b, name):
            if name == 'mtime':
                if USE_GFMKDIR_PLUS is False \
                   or USE_GFCHMOD_PLUS is False \
                   or USE_GFREG_PLUS is False:
                    # skip (not supported)
                    return
                a = int(a)
                b = int(b)
            elif name == 'mode':
                a = oct(a & 0o7777)
                b = oct(b & 0o7777)
            if a != b:
                raise Exception(
                    f'{ent1.path} vs {ent2.path}: prop={name}: {a} != {b}')
            logger.debug(f'GfURLEntry.compare:prop={name}: PASS')

        def cmpprop(ent1, ent2, properties):
            for pname in properties:
                val1 = getattr(ent1, pname)
                val2 = getattr(ent2, pname)
                cmpval(val1, val2, pname)

        # path is not compared
        cmpprop(ent1, ent2, ['file_type', 'mode', 'mtime', 'size', 'linkname'])
        if same_owner:
            cmpprop(ent1, ent2, ['uname', 'gname'])
        if data and ent1.is_file():
            url1 = GfURL.init(ent1.path)
            url2 = GfURL.init(ent2.path)
            if not url1.compare_data(url2, bufsize=bufsize):
                raise Exception(f'{ent1.path} vs {ent2.path}: different data')
            logger.debug('GfURLEntry.compare:data: PASS')
        return True


class GfURL(metaclass=abc.ABCMeta):
    MAXNAMLEN = 255  # SEE ALSO: dirent.h, gfarm/gfs.h (GFS_MAXNAMLEN)

    @classmethod
    def shutup_stderr(cls):
        shutup_stderr()

    @staticmethod
    def init(url, use_gfarm_command=False, local=False):
        if local:
            return GfURLLocal(url)
        if GfURLGfarm.is_my_URL(url):
            return GfURLGfarm(url)
        gfurl1 = GfURLLocal(url)
        if not use_gfarm_command:
            return gfurl1
        # use_gfarm_command=True: use gf* commands on gfarm2fs
        gfurl2 = gfurl1.get_gfarm_url_by_gfarm2fs()
        if gfurl2 is not None:
            # replace
            logger.debug('use Gfarm URL(%s) on gfarm2fs', gfurl2.url_str)
            return gfurl2
        return gfurl1

    def __init__(self, url):
        # allow_fragments=False : allow "#filename"
        self._url = urlparse(url, allow_fragments=False)

    @property
    def basename(self):
        return os.path.basename(self.path)

    @property
    def parent(self):
        parent_path = os.path.dirname(self.path)
        # "." ... relative path (of local path)
        # "/" ... absolute path
        if self.root_url_str != '' \
           and (parent_path == '.' or parent_path == '/'):
            parent_path = ''
        return GfURL.init(self.root_url_str + parent_path)

    @property
    def parent_list(self):
        def parents_func(u):
            if u.path != '.' and u.path != '/':
                yield u.parent
                yield from parents_func(u.parent)
                return
        yield from parents_func(self)

    @property
    def url_str(self):
        return self._url.geturl()

    @property
    def path(self):
        return os.path.normpath(self._url.path)

    @property
    def root_url_str(self):
        # ex. http://example.com/a/b/c -> http://example.com
        return self.url_str[:-len(self._url.path)]

    def subpath(self, fullpath):
        base = self.url_str
        if not fullpath.startswith(base):
            logger.error('subpath: %s, %s', base, fullpath)
            raise AssertionError
        logger.debug('subpath: %s, %s', base, fullpath)
        return fullpath[len(base):].lstrip('/')  # relative path

    def url_join(self, subpath):
        return os.path.join(self.url_str, subpath.lstrip('/'))

    def is_gfarm(self):
        return isinstance(self, GfURLGfarm)

    def is_local(self):
        return isinstance(self, GfURLLocal)

    def create_new_dir(self):
        if self.exists():
            raise FileExistsError(self.url_str)
        self.mkdir()
        if not self.is_directory():
            raise NotADirectoryError(self.url_str)
        if not self.is_empty():
            raise FileExistsError(self.url_str)

    @classmethod
    @abc.abstractmethod
    def is_my_URL(cls, url):
        raise NotImplementedError

    @abc.abstractmethod
    def chmod(self, mode, mtime=None, user=None, group=None,
              follow_symlinks=True):
        raise NotImplementedError

    @abc.abstractmethod
    def chown(self, user, group, follow_symlinks=True):
        raise NotImplementedError

    @abc.abstractmethod
    def utime(self, atime, mtime, follow_symlinks=True):
        raise NotImplementedError

    @abc.abstractmethod
    def mkdir(self, mode=0o700, parents=False):
        raise NotImplementedError

    def makedirs(self, mode=0o700):
        self.mkdir(mode, parents=True)

    @abc.abstractmethod
    def remove_tree(self, remove_readonly=False):
        raise NotImplementedError

    @abc.abstractmethod
    def symlink(self, linkname):
        raise NotImplementedError

    @abc.abstractmethod
    def hardlink(self, linkname):
        raise NotImplementedError

    @abc.abstractmethod
    def exists(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_directory(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_symlink(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_empty(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_readable(self):
        raise NotImplementedError

    @abc.abstractmethod
    def is_writable(self):
        raise NotImplementedError

    @abc.abstractmethod
    def get_size(self):
        raise NotImplementedError

    @abc.abstractmethod
    def listdir(self, path_only=False, recursive=False, first=False,
                hardlink_warn=True):
        raise NotImplementedError

    @contextmanager
    @abc.abstractmethod
    def readopen(self, textmode=False):
        raise NotImplementedError

    @contextmanager
    @abc.abstractmethod
    def writeopen(self, textmode=False, mode=0o600, mtime=None,
                  user=None, group=None, use_fsync=True, hostname=None):
        raise NotImplementedError

    def tar_add(self, tar, subpath, entry):
        tarinfo = entry.toTarinfo(subpath)
        if tarinfo is None:
            return
        # hard link is not supported
        if entry.is_file():
            url = GfURL.init(entry.path)
            if url.is_readable():
                with url.readopen() as f:
                    tar.addfile(tarinfo, fileobj=f)
            else:
                msg = f'(not readable) Permission denied: {url.url_str}'
                logger.debug(msg)
                raise Exception(msg)
        else:
            tar.addfile(tarinfo)

    def copy_from(self, inf, bufsize, mode=None, mtime=0o600,
                  user=None, group=None, use_fsync=True, hostname=None):
        readlen = 0
        with self.writeopen(mode=mode, mtime=mtime,
                            user=user, group=group,
                            use_fsync=use_fsync, hostname=hostname) as outf:
            while True:
                buf = inf.read(bufsize)
                if not buf:
                    break
                # binary mode
                wlen = outf.write(buf)
                readlen += wlen
        return readlen

    def sha256(self, bufsize=1048576):
        h = hashlib.sha256()
        with self.readopen() as f:
            while True:
                buf = f.read(bufsize)
                if not buf:
                    break
                h.update(buf)
        return h.hexdigest()

    def compare_data(self, url2, bufsize=1048576):
        url1 = self
        return url1.sha256(bufsize) == url2.sha256(bufsize)

    def compare_entries(self, d2url, data=True, same_owner=False,
                        bufsize=1048576, exclude_list=None):
        def search_compare_pop(subpath, ent1, dirdict2):
            ent2 = dirdict2.pop(subpath, None)
            if ent2 is None:
                return False  # not found
            ent1.compare(ent2, data=data, same_owner=same_owner,
                         bufsize=bufsize)
            # raise if different
            return True

        def excluded(path):
            if exclude_list is None:
                return False
            for exclude in exclude_list:
                if re.match(exclude, path):
                    return True
            return False

        def listdir_ignore_error(gfurl):
            try:
                yield from gfurl.listdir(recursive=True)
            except Exception as e:
                logger.warning('listdir(%s): error ignored: %s',
                               gfurl.url_str, str(e))

        d1url = self
        # iterator
        d1i = listdir_ignore_error(d1url)
        d2i = listdir_ignore_error(d2url)
        d1dict = dict()
        d2dict = dict()
        d2stop = False
        for d1ent in d1i:
            subpath1 = d1ent.subpath(d1url)
            p1base = os.path.basename(subpath1)
            if excluded(p1base):
                continue
            logger.debug('compare:dir1: %s', subpath1)
            try:
                if not search_compare_pop(subpath1, d1ent, d2dict):
                    # not found, compare later
                    d1dict[subpath1] = d1ent
            except Exception as e:
                logger.error(str(e))
                return False
            while not d2stop:
                d2ent = next(d2i, None)
                if d2ent is None:
                    d2stop = True
                    break
                subpath2 = d2ent.subpath(d2url)
                p2base = os.path.basename(subpath2)
                if excluded(p2base):
                    continue
                logger.debug('compare:dir2: %s', subpath2)
                try:
                    if not search_compare_pop(subpath2, d2ent, d1dict):
                        # not found, compare later
                        d2dict[subpath2] = d2ent
                except Exception as e:
                    logger.error(str(e))
                    return False
        if d2stop is False:
            while True:
                d2ent = next(d2i, None)
                if d2ent is None:
                    break
                subpath2 = d2ent.subpath(d2url)
                p2base = os.path.basename(subpath2)
                if excluded(p2base):
                    continue
                logger.debug('compare:dir2: %s', subpath1)
                d2dict[subpath2] = d2ent
        d1notfound = dict()
        for subpath1, d1ent in d1dict.items():
            try:
                if not search_compare_pop(subpath1, d1ent, d2dict):
                    d1notfound[subpath1] = d1ent
            except Exception as e:
                logger.error(str(e))
                return False
        if len(d1notfound) > 0 or len(d2dict) > 0:
            logger.error('different entries: dir1=%s, dir2=%s',
                         pformat(d1notfound), pformat(d2dict))
            return False
        logger.debug('compare_entries: PASS')
        return True


def str2bool(s):
    return s.upper() in ['TRUE', '1', 'ON', 'ENABLE', 'ENABLED']


USE_GFMKDIR_PLUS = str2bool(os.getenv('GFMKDIR_PLUS', 'True'))
USE_GFCHMOD_PLUS = str2bool(os.getenv('GFCHMOD_PLUS', 'True'))
USE_GFREG_PLUS = str2bool(os.getenv('GFREG_PLUS', 'True'))


class GfURLGfarm(GfURL):
    # Ex. 12345 -rw-rw-r-- 1 user1  gfarmadm     29 Jan  1 00:00:00 2022 fname
    PAT_ENTRY = re.compile(r'^\s*(\d+)\s+([-dl]\S+)\s+(\d+)\s+(\S+)\s+(\S+)\s+'
                           r'(\d+)\s+(\S+\s+\d+\s+\d+:\d+:\d+\s+\d+)\s+(.+)$')
    PAT_EMPTY = re.compile(r'^\s*$')

    def __init__(self, url):
        super().__init__(url)

    @classmethod
    def is_my_URL(cls, url):
        return url.startswith('gfarm:')

    @staticmethod
    def to_oct_str(mode):
        # ex. (int)0o644 -> (str)'644'
        return oct(mode)[2:]

    def chmod(self, mode, mtime=None, user=None, group=None,
              follow_symlinks=True):
        args = ['gfchmod']
        if USE_GFCHMOD_PLUS:
            if mtime is not None:
                args.append('-M')
                args.append(str(int(mtime)))
            if user is not None:
                args.append('-u')
                args.append(user)
            if group is not None:
                args.append('-g')
                args.append(group)
        if not follow_symlinks:
            args.append('-h')
        args.append(self.to_oct_str(mode))
        args.append(self.url_str)
        execcmd(args)
        if not USE_GFCHMOD_PLUS:
            if mtime is not None:
                self.utime(mtime, mtime, follow_symlinks=follow_symlinks)
            self.chown(user, group, follow_symlinks=follow_symlinks)

    def chown(self, user, group, follow_symlinks=True):
        if user is None and group is None:
            return
        chown_args = ['gfchown']
        if not follow_symlinks:
            chown_args.append('-h')
        if user is not None and group is not None:
            chown_args.append(user + ':' + group)
        elif user is not None:
            chown_args.append(user)
        elif group is not None:
            chown_args.append(':' + group)
        chown_args.append(self.url_str)
        execcmd(chown_args)

    def utime(self, atime, mtime, follow_symlinks=True):
        # not used when USE_GFCHMOD_PLUS == True
        logger.warning('mtime cannot be updated (not supported): %s',
                       self.url_str)

    def mkdir(self, mode=0o700, parents=False):
        args = ['gfmkdir']
        if parents:
            args.append('-p')
        if USE_GFMKDIR_PLUS:
            if mode is not None:
                args.append('-m')
                args.append(self.to_oct_str(mode))
            args.append(self.url_str)
            execcmd(args)
        else:
            args.append(self.url_str)
            execcmd(args)
            if mode is not None:
                # slow
                self.chmod(mode)

    def remove_tree(self, remove_readonly=False):
        path = self.url_str
        if path == '/' or path == '':
            raise AssertionError
        try:
            execcmd(['gfrm', '-rf', path])
        except Exception:
            if remove_readonly:
                execcmd(['gfchmod', '-R', '700', path])
                execcmd(['gfrm', '-rf', path])
            else:
                raise

    def symlink(self, linkname):
        execcmd(['gfln', '-s', linkname, self.url_str])

    def hardlink(self, linkname):
        execcmd(['gfln', linkname, self.url_str])

    def exists(self):
        out, err, ret = execcmd_raw(['gftest', '-e', self.url_str])
        return ret == 0

    def is_directory(self):
        out, err, ret = execcmd_raw(['gftest', '-d', self.url_str])
        return ret == 0

    def is_symlink(self):
        out, err, ret = execcmd_raw(['gftest', '-h', self.url_str])
        return ret == 0

    def is_empty(self):
        count = 0
        for line in execcmd_readline(['gfls', '-1a', self.url_str]):
            if line is None:
                continue
            if line == '.' or line == '..':
                continue
            count += 1
        logger.debug('is_empty: %s, %d', self.url_str, count)
        return count == 0

    R_OK = 0o4
    W_OK = 0o2
    X_OK = 0o1

    def access(self, mode):
        out, err, ret = execcmd_raw(['gfxattr', '-g', self.url_str,
                                     'gfarm.effective_perm'])
        if ret == 0:
            val = int.from_bytes(out, byteorder='big')
            return val & mode > 0
        raise Exception(f'gfarm.effective_perm({self.url_str}): {err}')

    def is_readable(self):
        return self.access(self.R_OK)

    def is_writable(self):
        return self.access(self.W_OK)

    def get_size(self):
        for entry in self.listdir(recursive=False):
            logger.debug('get_size: %s, %d', entry.path, entry.size)
            return entry.size

    @classmethod
    def from_rwx(cls, rwx, highchar):
        perm = 0
        highbit = 0
        r = rwx[0]
        w = rwx[1]
        x = rwx[2]
        if r == 'r':
            perm |= 0o4
        if w == 'w':
            perm |= 0o2
        if x == 'x':
            perm |= 0o1
        elif x == highchar:
            perm |= 0o1
            highbit = 0o1
        elif x == highchar.upper():
            highbit = 0o1
        return perm, highbit

    @classmethod
    def mode_convert(cls, mode_str, name):
        typestr = mode_str[0]
        file_type = None
        if typestr == 'd':
            file_type = GfURLEntry.TYPE_DIR
        elif typestr == 'l':
            file_type = GfURLEntry.TYPE_SYMLINK
        elif typestr == '-':
            file_type = GfURLEntry.TYPE_FILE
        else:
            logger.warning('unsupported type: %s, %s', name, typestr)

        mode = 0
        perm, highbit = cls.from_rwx(mode_str[1:4], 's')
        mode |= (perm << 6)
        mode |= (highbit << 11)
        perm, highbit = cls.from_rwx(mode_str[4:7], 's')
        mode |= (perm << 3)
        mode |= (highbit << 10)
        perm, highbit = cls.from_rwx(mode_str[7:10], 't')
        mode |= perm
        mode |= (highbit << 9)
        return mode, file_type

    def listdir(self, path_only=False, recursive=False, first=False,
                hardlink_warn=False):
        dirname = self.url_str
        gfls_opt = '-ailT'
        if recursive:
            gfls_opt += 'R'
        for line in execcmd_readline(['gfls', gfls_opt, self.url_str]):
            logger.debug('listdir: raw line=%s', line)
            if line is None:
                continue
            if self.PAT_EMPTY.match(line):
                continue
            m = self.PAT_ENTRY.match(line)
            if m:
                # Ex.
                # 12345 -rw-rw-r-- 1 user1 group1 29 Jan 1 00:00:00 2022 fname
                inum = int(m.group(1))
                mode_str = m.group(2)
                nlink = int(m.group(3))
                uname = m.group(4)
                gname = m.group(5)
                size = int(m.group(6))
                mtime_str = m.group(7)
                name = m.group(8)

                if name == '..':
                    continue
                if not first and name == '.':
                    continue
                mtime = time.mktime(
                    time.strptime(mtime_str, '%b %d %H:%M:%S %Y'))
                mode, file_type = self.mode_convert(mode_str, name)
                if file_type is None:
                    continue
                linkname = ''
                if file_type == GfURLEntry.TYPE_SYMLINK:
                    link = name.split(' -> ')
                    name = link[0]
                    linkname = link[1]
                elif file_type == GfURLEntry.TYPE_FILE:
                    if hardlink_warn and nlink >= 2:
                        logger.warning('hard link is not supported: '
                                       'nlink=%d, inode=%d (Gfarm): %s',
                                       nlink, inum, name)
                if first and name == '.':
                    path = dirname
                elif '/' in name:  # "name" is fullpath of a file
                    path = name
                else:
                    path = dirname + '/' + name
                # path is Gfarm URL (gfarm:/...)
                start = self
                yield GfURLEntry(start, path, mode, file_type, uname, gname,
                                 size, mtime, linkname)
            else:
                # ex. gfarm:/home/user1/dir: -> gfarm:/home/user1/dir
                dirname = line[:-1]
                if not dirname.startswith(self.url_str):
                    raise AssertionError
                first = False

    def gfsched(self, is_file=False, write_mode=False, number=None):
        args = ['gfsched']
        if is_file:
            args.append('-f')
            args.append(self.url_str)
        else:
            args.append('-P')
            args.append(self.url_str)
        if write_mode is not None:
            args.append('-w')
        if number is not None:
            args.append('-n')
            args.append(str(number))
        return [line for line in execcmd_readline(args)]

    def gfreg(self, textmode=False, mode=None, mtime=None,
              user=None, group=None, hostname=None):
        class Gfreg():
            def __init__(self, url, proc, stdin):
                self.url = url
                self.proc = proc
                self.stdin = stdin

            def close(self):
                self.stdin.close()

            def post(self):
                if USE_GFREG_PLUS:
                    return
                # must be called after wait()
                if mode is not None:
                    self.url.chmod(mode, mtime=mtime, user=user, group=group)
                else:
                    if mtime is not None:
                        self.url.utime(mtime, mtime)
                    self.url.chown(user, group)

        args = ['gfreg']
        if USE_GFREG_PLUS:
            if mode is not None:
                args.append('-m')
                args.append(self.to_oct_str(mode))
            if mtime is not None:
                args.append('-M')
                args.append(str(int(mtime)))
            if user is not None:
                args.append('-u')
                args.append(user)
            if group is not None:
                args.append('-g')
                args.append(group)
        if hostname:
            args.append('-h')
            args.append(hostname)
        args.append('-')  # stdin
        args.append(self.url_str)
        if textmode:
            encoding = get_encoding()
        else:
            encoding = None
        proc = subprocess.Popen(
            args, shell=False, encoding=encoding, close_fds=True,
            stdin=subprocess.PIPE, stdout=subprocess.DEVNULL,
            stderr=DEFAULT_STDERR)
        return Gfreg(self, proc, proc.stdin)

    def gfexport(self, textmode=False):
        args = ['gfexport', self.url_str]
        if textmode:
            encoding = get_encoding()
        else:
            encoding = None
        return subprocess.Popen(
            args, shell=False, encoding=encoding, close_fds=True,
            stdin=subprocess.DEVNULL, stdout=subprocess.PIPE,
            stderr=DEFAULT_STDERR)

    @contextmanager
    def readopen(self, textmode=False):
        proc = self.gfexport(textmode=textmode)
        try:
            yield proc.stdout
        finally:
            proc.stdout.close()
            ret = proc.wait()
            if ret != 0:
                raise Exception('{}: returncode={}'.format(
                    ' '.join(proc.args), ret))

    @contextmanager
    def writeopen(self, textmode=False, mode=0o600, mtime=None,
                  user=None, group=None, use_fsync=True, hostname=None):
        # TODO XXX add gfreg option for use_fsync
        gfreg_obj = self.gfreg(textmode=textmode, mode=mode, mtime=mtime,
                               user=user, group=group, hostname=hostname)
        proc = gfreg_obj.proc
        try:
            yield gfreg_obj.stdin
        finally:
            gfreg_obj.close()
            ret = proc.wait()
            if ret != 0:
                raise Exception('{}: returncode={}'.format(
                    ' '.join(proc.args), ret))
            gfreg_obj.post()


def get_uid(user):
    try:
        pw = pwd.getpwnam(user)
        if pw is not None:
            return pw.pw_uid
    except KeyError:
        pass
    return -1  # uid is not changed by os.chown()


def get_gid(group):
    try:
        gr = grp.getgrnam(group)
        if gr is not None:
            return gr.gr_gid
    except KeyError:
        pass
    return -1  # gid is not changed by os.chown()


class GfURLLocal(GfURL):
    def __init__(self, url):
        super().__init__(url)

    @classmethod
    def is_my_URL(cls, url):
        return True

    def get_gfarm_url_by_gfarm2fs(self):
        # SEE ALSO: lib/libgfarm/gfarm/gfarm_path.c
        XATTR_GFARM2FS_URL = 'gfarm2fs.url'
        try:
            val = os.getxattr(self.url_str, XATTR_GFARM2FS_URL)
            gfurl = GfURL.init(val.decode(get_encoding()))
            if gfurl.is_gfarm():
                return gfurl
            raise Exception('unexpected Gfarm URL format: ' + gfurl.url_str)
        except OSError as e:
            if e.errno == errno.EOPNOTSUPP:
                return None
            elif e.errno == errno.ENOENT:
                # creation mode
                try:
                    val = os.getxattr(self.parent.url_str, XATTR_GFARM2FS_URL)
                    parent_str = val.decode(get_encoding())
                    gfurl = GfURL.init(os.path.join(parent_str, self.basename))
                    if gfurl.is_gfarm():
                        return gfurl
                    raise Exception('unexpected Gfarm URL format: '
                                    + gfurl.url_str)
                except OSError as e2:
                    if e2.errno == errno.EOPNOTSUPP:
                        # local filesystem
                        return None
                    raise
                except Exception:
                    raise
            else:
                raise
        except Exception:
            raise

    def chmod(self, mode, mtime=None, user=None, group=None,
              follow_symlinks=True):
        if follow_symlinks:
            os.chmod(self.url_str, mode)
        else:
            try:
                os.chmod(self.url_str, mode, follow_symlinks=False)
            except NotImplementedError:
                if not self.is_symlink():
                    raise
                # ignore, not changed for symlink
        if mtime is not None:
            self.utime(mtime, mtime, follow_symlinks=follow_symlinks)
        self.chown(user, group)

    def chown(self, user, group, follow_symlinks=False):
        if user is not None or group is not None:
            # shutil.chown does not have follow_symlinks option.
            uid = get_uid(user)
            gid = get_gid(group)
            os.chown(self.url_str, uid, gid, follow_symlinks=follow_symlinks)

    def utime(self, atime, mtime, follow_symlinks=True):
        os.utime(self.url_str, times=(atime, mtime),
                 follow_symlinks=follow_symlinks)

    def mkdir(self, mode=0o700, parents=False):
        if parents:
            os.makedirs(self.url_str, mode=mode, exist_ok=True)
        else:
            os.mkdir(self.url_str, mode)

    def _chmod_recursive(self, path, dir_mode, file_mode):
        for root, dirs, files in os.walk(path):
            for d in dirs:
                dir_path = os.path.join(root, d)
                os.chmod(dir_path, dir_mode)
            for f in files:
                file_path = os.path.join(root, f)
                os.chmod(file_path, file_mode)

    def remove_tree(self, remove_readonly=False):
        path = self.path
        if path == '/' or path == '':
            raise AssertionError
        if remove_readonly:
            self._chmod_recursive(path, 0o700, 0o600)
        shutil.rmtree(path)

    def symlink(self, linkname):
        os.symlink(linkname, self.url_str)

    def hardlink(self, linkname):
        os.link(linkname, self.url_str)

    def exists(self):
        return os.path.exists(self.url_str)

    def is_directory(self):
        return os.path.isdir(self.url_str)

    def is_symlink(self):
        return os.path.islink(self.url_str)

    def is_empty(self):
        return len(os.listdir(self.url_str)) == 0

    def is_readable(self):
        return os.access(self.url_str, os.R_OK)

    def is_writable(self):
        return os.access(self.url_str, os.W_OK)

    def get_size(self):
        st = os.stat(self.url_str, follow_symlinks=False)
        logger.debug('get_size: %s, %d', self.url_str, st.st_size)
        return st.st_size

    @classmethod
    def _readlink(cls, path, is_symlink):
        if is_symlink:
            linkname = os.readlink(path)
        else:
            linkname = ''
        return linkname

    @classmethod
    def uid2name(cls, uid):
        try:
            pw = pwd.getpwuid(uid)
            return pw.pw_name
        except Exception:
            return str(uid)

    @classmethod
    def gid2name(cls, gid):
        try:
            gr = grp.getgrgid(gid)
            return gr.gr_name
        except Exception:
            return str(gid)

    @classmethod
    def _toFileType(cls, st):
        file_type = GfURLEntry.TYPE_OTHER
        if stat.S_ISREG(st.st_mode):
            file_type = GfURLEntry.TYPE_FILE
        elif stat.S_ISDIR(st.st_mode):
            file_type = GfURLEntry.TYPE_DIR
        elif stat.S_ISLNK(st.st_mode):
            file_type = GfURLEntry.TYPE_SYMLINK
        return file_type

    def _toGfURLEntry(self, entry, path_only, hardlink_warn):
        start = self
        if path_only:
            return GfURLEntry(start, entry.path, 0, None,
                              'root', 'root',
                              0, 0, '')
        st = entry.stat(follow_symlinks=False)
        file_type = self._toFileType(st)
        if file_type == GfURLEntry.TYPE_FILE:
            if hardlink_warn and st.st_nlink >= 2:
                logger.warning('hard link is not supported: '
                               'nlink=%d, inode=%d (Local): %s',
                               st.st_nlink, st.st_ino, entry.path)
        linkname = self._readlink(entry.path, entry.is_symlink())
        return GfURLEntry(start, entry.path, st.st_mode, file_type,
                          self.uid2name(st.st_uid),
                          self.gid2name(st.st_gid),
                          st.st_size, st.st_mtime, linkname)

    def _scandir(self, path, path_only, recursive, first, hardlink_warn):
        if first:
            st = os.stat(path, follow_symlinks=True)
            start = self
            yield GfURLEntry(start, path, st.st_mode, self._toFileType(st),
                             self.uid2name(st.st_uid),
                             self.gid2name(st.st_gid),
                             st.st_size, st.st_mtime, '')
            file_type = self._toFileType(st)
            if file_type != GfURLEntry.TYPE_DIR:
                return

        first = False
        has_error = None
        try:
            for entry in os.scandir(path):
                yield self._toGfURLEntry(entry, path_only, hardlink_warn)
                if recursive and entry.is_dir(follow_symlinks=False):
                    try:
                        yield from self._scandir(entry.path, path_only,
                                                 recursive,
                                                 first, hardlink_warn)
                    except Exception as e2:
                        if has_error is None:  # save first error
                            has_error = e2
                        # DO NOT raise here
        except Exception as e:
            if has_error is None:  # save first error
                has_error = e
                logger.warning(str(e))  # logging only once
        if has_error is not None:
            raise has_error

    def listdir(self, path_only=False, recursive=False, first=False,
                hardlink_warn=True):
        yield from self._scandir(self.url_str, path_only, recursive, first,
                                 hardlink_warn)

    # NOTE: This is not expected behavior.
    # - This can copy a hard link,
    #   but a hard link cannot be extracted from gfexport (stream open)
    # - When the specified <member> for --create is a symlink,
    #   the entry will be archived as symlink.
    # def tar_add(self, tar, subpath, entry):
    #     if entry.path:
    #         path = os.path.join(self.url_str, entry.path)
    #     else:
    #         path = self.url_str
    #     tar.add(path, arcname=subpath, recursive=False)

    @contextmanager
    def readopen(self, textmode=False):
        if textmode:
            f = open(self.url_str, 'rt', encoding=get_encoding())
        else:
            f = open(self.url_str, 'rb')
        try:
            yield f
        finally:
            f.close()

    @contextmanager
    def writeopen(self, textmode=False, mode=0o600, mtime=None,
                  user=None, group=None, use_fsync=True, hostname=None):
        tmpmode = mode | 0o200  # necessary (Permission denied at ex.0o400)
        fd = os.open(path=self.url_str,
                     flags=(os.O_WRONLY | os.O_CREAT | os.O_EXCL),
                     mode=tmpmode,
                     )
        if textmode:
            f = open(fd, 'wt', encoding=get_encoding(), closefd=True)
        else:
            f = open(fd, 'wb', closefd=True)
        try:
            yield f
        finally:
            if use_fsync:
                f.flush()
                os.fsync(f.fileno())
            f.close()
            if mode is not None:
                self.chmod(mode, mtime=mtime, user=user, group=group)
            else:
                if mtime is not None:
                    self.utime(mtime, mtime)
                self.chown(user, group)


class GfTarFile(tarfile.TarFile):
    COMPRESS_TYPE_NO = 'no'
    ATTR_PROC_LIST = '_gfptar_proc_list'  # [(proc, fileobj, fileobj), ...]
    USE_FSYNC = 'use_fsync'

    @classmethod
    def extract_open(cls, gfurl, copybufsize, compress_prog=None):
        # use Stream (not seekable)
        if compress_prog is not None:
            openmode = 'r|'
        else:
            openmode = 'r|*'  # any type (gz, bz2, xz)
        if not gfurl.exists():
            raise FileNotFoundError(gfurl.url_str)
        # list of tuple(proc, closeable obj, synchronizable obj)
        proc_list = []
        if gfurl.is_gfarm():
            if compress_prog:
                gfexport_proc = gfurl.gfexport()
                decompress_proc = cls.decompress(compress_prog,
                                                 gfexport_proc.stdout)
                tar = cls.open(None, mode=openmode,
                               fileobj=decompress_proc.stdout,
                               copybufsize=copybufsize)
                proc_list.append(tuple([gfexport_proc,
                                        gfexport_proc.stdout, None]))
                proc_list.append(tuple([decompress_proc,
                                        decompress_proc.stdout, None]))
            else:
                gfexport_proc = gfurl.gfexport()
                tar = cls.open(None, mode=openmode,
                               fileobj=gfexport_proc.stdout,
                               copybufsize=copybufsize)
                proc_list.append(tuple([gfexport_proc,
                                        gfexport_proc.stdout, None]))
        else:
            if compress_prog:
                inf = open(gfurl.url_str, 'rb')
                decompress_proc = cls.decompress(compress_prog, inf)
                tar = cls.open(None, mode=openmode,
                               fileobj=decompress_proc.stdout,
                               copybufsize=copybufsize)
                proc_list.append(tuple([None, inf, None]))
                proc_list.append(tuple([decompress_proc,
                                        decompress_proc.stdout, None]))
            else:
                tar = GfTarFile.open(gfurl.url_str, mode=openmode,
                                     copybufsize=copybufsize)
        setattr(tar, cls.ATTR_PROC_LIST, proc_list)
        return tar

    @classmethod
    def create_open(cls, gfurl, compress_type, copybufsize, compress_prog=None,
                    use_fsync=True, target_host=None):
        # use Stream (not seekable)
        openmode = 'w|'
        if compress_prog is None \
           and compress_type != cls.COMPRESS_TYPE_NO:
            openmode = 'w|' + compress_type
        if gfurl.exists():
            raise FileExistsError(gfurl.url_str)
        # list of tuple(proc, closeable obj, synchronizable obj)
        proc_list = []
        if gfurl.is_gfarm():
            if compress_prog:
                gfreg_obj = gfurl.gfreg(mode=0o600, hostname=target_host)
                compress_proc = cls.compress(compress_prog,
                                             gfreg_obj.stdin)
                tar = cls.open(None, mode=openmode,
                               fileobj=compress_proc.stdin,
                               copybufsize=copybufsize)
                proc_list.append(tuple([compress_proc, compress_proc.stdin,
                                        None]))
                proc_list.append(tuple([gfreg_obj.proc, gfreg_obj, None]))
            else:
                gfreg_obj = gfurl.gfreg(mode=0o600, hostname=target_host)
                tar = cls.open(None, mode=openmode,
                               fileobj=gfreg_obj.stdin,
                               copybufsize=copybufsize)
                proc_list.append(tuple([gfreg_obj.proc, gfreg_obj, None]))
        else:  # Local
            if compress_prog:
                outf = open(gfurl.url_str, 'wb')
                compress_proc = cls.compress(compress_prog, outf)
                tar = cls.open(None, mode=openmode,
                               fileobj=compress_proc.stdin,
                               copybufsize=copybufsize)
                proc_list.append(tuple([compress_proc, compress_proc.stdin,
                                        None]))
                proc_list.append(tuple([None, outf, outf]))
            else:
                outf = open(gfurl.url_str, 'wb')
                tar = cls.open(gfurl.url_str, mode=openmode,
                               fileobj=outf, copybufsize=copybufsize)
                proc_list.append(tuple([None, outf, outf]))
            gfurl.chmod(0o600)
        logger.debug('GfTarFile.create_open: %s', gfurl.url_str)
        setattr(tar, cls.ATTR_PROC_LIST, proc_list)
        setattr(tar, cls.USE_FSYNC, use_fsync)
        return tar

    @classmethod
    def compress(cls, compress_prog, outf):
        args = [compress_prog]
        # binary mode
        return subprocess.Popen(
            args, shell=False, close_fds=True,
            stdin=subprocess.PIPE, stdout=outf,
            stderr=DEFAULT_STDERR)

    @classmethod
    def decompress(cls, compress_prog, inf):
        args = [compress_prog, '-d']
        # binary mode
        return subprocess.Popen(
            args, shell=False, close_fds=True,
            stdin=inf, stdout=subprocess.PIPE,
            stderr=DEFAULT_STDERR)

    # override
    def close(self):
        super().close()
        use_fsync = getattr(self, self.USE_FSYNC, True)
        proc_list = getattr(self, self.ATTR_PROC_LIST, None)
        if proc_list:
            for proc_tuple in proc_list:
                proc, close_obj, sync_obj = proc_tuple
                if use_fsync and sync_obj:
                    sync_obj.flush()
                    os.fsync(sync_obj.fileno())
                if close_obj:
                    close_obj.close()
                if proc is not None:
                    logger.debug('close external process for tar: %s',
                                 str(proc.args))
                    ret = proc.wait()
                    if ret != 0:
                        raise Exception('{}: returncode={}'.format(
                            ' '.join(proc.args), ret))
                if close_obj:
                    post_func = getattr(close_obj, 'post', None)
                    if post_func:
                        close_obj.post()

    def add_entry(self, subpath, entry):
        entry.start_url.tar_add(self, subpath, entry)


class TestGfptar(unittest.TestCase):
    @staticmethod
    def suite():
        suite = unittest.TestSuite()
        suite.addTest(TestGfptar('test_unhumanize'))
        suite.addTest(TestGfptar('test_GfURL_use_gfarm_command_for_local'))
        return suite

    def test_unhumanize(self):
        self.assertEqual(unhumanize_number('1K'), 1024)
        self.assertEqual(unhumanize_number('2M'), 2097152)
        self.assertEqual(unhumanize_number('3G'), 3221225472)
        self.assertEqual(unhumanize_number('4T'), 4398046511104)
        self.assertEqual(unhumanize_number('5P'), 5629499534213120)
        self.assertEqual(unhumanize_number('6E'), 6917529027641081856)

    def test_GfURL_use_gfarm_command_for_local(self):
        url = GfURL.init('/tmp', use_gfarm_command=True)
        self.assertEqual(url.is_local(), True)
        self.assertEqual(url.path, '/tmp')


class GfptarCommand(Command):
    LIST_SUFFIX = '.lst'

    def __init__(self, name):
        self.init(name)
        self.canceled = threading.Event()
        self.lock_init(False)
        self.futures = None
        self.sig_init()
        self.hardlink_warn = True
        if self.quiet:
            GfURL.shutup_stderr()

    def options_init(self):
        set_encoding(self.opt['--encoding'])
        self.jobs = self.opt['--jobs']
        self.bufsize = self.opt['--bufsize']
        self.progress_enabled = self._progress_enabled()
        self.use_fsync = not self.opt['--disable-fsync']

    def sig_init(self):
        def sig_handler(signum, frame):
            print('')  # new line
            self.cancel()

        signal.signal(signal.SIGINT, sig_handler)
        signal.signal(signal.SIGTERM, sig_handler)

    def getDoc(self) -> str:
        return __doc__

    def getSchema(self) -> Schema:
        return _schema

    def _progress_enabled(self):
        return not self.debug and not self.verbose and not self.quiet

    def run(self):
        self.logger.debug(pformat(self.opt))
        try:
            outdir = self.opt['--create']
            if outdir:
                basedir = self.opt['--basedir']
                infiles = self.opt['<member>']
                self.create(outdir, basedir, infiles)
                return

            outdir = self.opt['--extract']
            if outdir:
                indir = self.opt['<indir>']
                members = self.opt['<member>']
                self.extract(outdir, indir, members)
                return

            indir = self.opt['--list']
            if indir:
                if self.verbose:
                    self.list_verbose(indir)
                else:
                    self.list_simple(indir)
                return

            if self.opt['--test']:
                self.test_main()
                return
        except Exception as e:
            if self.debug:
                raise
            else:
                if type(e) == Exception:  # custom errors
                    logger.error(str(e))
                else:
                    logger.error('%s: %s', e.__class__.__name__, str(e))
                sys.exit(1)

    def test_main(self):
        self.am_I_gfarmroot = am_I_gfarmroot()
        if self.am_I_gfarmroot:
            logger.warning('gfarmroot TEST: "noread" files are not used')
        self.options_init()
        self.hardlink_warn = False
        self.uid = os.getuid()
        self.pid = os.getpid()
        out = gfwhoami()
        self.gfarm_user = out.strip()
        self.test_unit()
        self.test_invalid('url', 'gfarm:/tmp', 'dst', True)
        self.test_invalid('dot1', '.', 'dst', True)
        self.test_invalid('dot2', '', 'dst', False)
        self.test_invalid('dot3', './', 'dst', False)
        self.test_invalid('dot4', '././././', 'dst', False)
        self.test_invalid('dotdot1', '..', 'dst', True)
        self.test_invalid('dotdot2', '../', 'dst', False)
        self.test_invalid('dotdot3', '../abc', 'dst', False)
        self.test_invalid('dotdot4', './..', 'dst', False)
        self.test_member()
        self.test_opt_pattern()
        self.test_specified_dir()

    def test_unit(self):
        verbosity = 2
        runner = unittest.TextTestRunner(verbosity=verbosity)
        result = runner.run(TestGfptar.suite())
        if len(result.errors) > 0 or len(result.failures) > 0:
            logger.error_exit(1, 'unittest error')
        print('unittest ... PASS')

    def test_opt_pattern(self):
        save_opt_size = self.opt['--size']
        save_opt_jobs = self.opt['--jobs']
        save_opt_type = self.opt['--type']
        save_opt_compress_prog = self.opt['--use-compress-program']

        # create tar per one entry
        self.opt['--size'] = 0
        pattern_jobs = [1, 10]
        for jobs in pattern_jobs:
            self.opt['--jobs'] = jobs
            self.test_simple('jobs_' + str(jobs), use_all_files=True)
        self.opt['--jobs'] = save_opt_jobs

        # create one tar
        self.opt['--size'] = unhumanize_number('100M')
        pattern_type = [
            'gz',
            # 'bz2',
            # 'xz',
            'no']
        for t in pattern_type:
            self.opt['--type'] = t
            self.test_simple('type_' + t)
        self.opt['--type'] = save_opt_type

        pattern_compress_prog = {
            # 'gz': 'gzip',
            # 'bz2': 'bzip2',
            'xz': 'xz',
            # 'lzip': 'lzip',
            # 'lzop': 'lzop',
        }
        for t, prog in pattern_compress_prog.items():
            w = shutil.which(prog)
            if not w:
                logger.error('SKIPPED: No such command: %s', prog)
                continue
            self.opt['--type'] = t
            self.opt['--use-compress-program'] = prog
            self.test_simple('compress_prog_' + prog)
        # self.opt['--use-compress-program'] = save_opt_compress_prog

        self.opt['--size'] = save_opt_size
        self.opt['--jobs'] = save_opt_jobs
        self.opt['--type'] = save_opt_type
        self.opt['--use-compress-program'] = save_opt_compress_prog

    def test_invalid(self, name, src, dst, for_gfarm):
        testname = f'gfptar-test-invalid-{name}'
        d_name = f'{testname}-{self.gfarm_user}-{self.uid}-{self.pid}'

        if for_gfarm:
            workdir = os.path.join(self.opt['--test-workdir-gfarm'], d_name)
        else:
            workdir = os.path.join(self.opt['--test-workdir-local'], d_name)
        workdir_url = GfURL.init(workdir)
        workdir_url.mkdir()

        testsrc_name = src
        # srcdir = workdir_url.url_join(testsrc_name)
        # srcdir_url = GfURL.init(srcdir)
        # srcdir_url.mkdir()

        test1_name = dst
        test1_dir = workdir_url.url_join(test1_name)

        ok = False
        try:
            self.create(test1_dir, workdir, [testsrc_name])
        except Exception as e:
            if str(e).startswith('specifying '):
                ok = True
            else:
                raise

        workdir_url.remove_tree(remove_readonly=True)
        if ok:
            print(testname + ' ... PASS')
        else:
            print(testname + ' ... FAIL (unexpected success)')

    def test_simple(self, name, use_all_files=False):
        testname = f'gfptar-test-simple-{name}'
        d_name = f'{testname}-{self.gfarm_user}-{self.uid}-{self.pid}'
        workdir_local = os.path.join(self.opt['--test-workdir-local'], d_name)
        workdir_gfarm = os.path.join(self.opt['--test-workdir-gfarm'], d_name)
        workdir_local_url = GfURL.init(workdir_local)
        workdir_gfarm_url = GfURL.init(workdir_gfarm)
        workdir_local_url.mkdir()
        workdir_gfarm_url.mkdir()

        testsrc_name = 'test-src'
        srcdir_local = workdir_local_url.url_join(testsrc_name)
        srcdir_gfarm = workdir_gfarm_url.url_join(testsrc_name)
        if use_all_files:
            readonly = True
            if self.am_I_gfarmroot:
                noread = False
            else:
                noread = True
            link = True
            longname = True
        else:
            readonly = False
            noread = False
            link = False
            longname = False
        self.test_prepare_srcdir(srcdir_local,
                                 readonly, noread, link, longname)
        self.test_prepare_srcdir(srcdir_gfarm,
                                 readonly, noread, link, longname)

        test1_name = 'test-1-create'
        test1_dir_gfarm = workdir_gfarm_url.url_join(test1_name)
        test2_name = 'test-2-extract'
        test2_dir_gfarm = workdir_gfarm_url.url_join(test2_name)
        test3_name = 'test-3-create'
        test3_dir_local = workdir_local_url.url_join(test3_name)
        test4_name = 'test-4-extract'
        test4_dir_local = workdir_local_url.url_join(test4_name)

        # Gfarm -> Gfarm(tar)
        try:
            self.create(test1_dir_gfarm, workdir_gfarm, [testsrc_name])
            if noread:
                logger.error_exit(1, testname + '(create:Gfarm->Gfarm) ' +
                                  '... FAIL (unexpected success)')
        except Exception:
            if not noread:
                raise
        # Gfarm(tar) -> Gfarm
        self.extract(test2_dir_gfarm, test1_dir_gfarm, [])
        # Gfarm -> Local(tar)
        try:
            self.create(test3_dir_local, test2_dir_gfarm, [testsrc_name])
            if noread:
                logger.error_exit(1, testname + '(create:Gfarm->Local) ' +
                                  '... FAIL (unexpected success)')
        except Exception:
            if not noread:
                raise
        # Local(tar) -> Local
        self.extract(test4_dir_local, test3_dir_local, [])

        # --list
        self.list_simple(test1_dir_gfarm, quiet=True)
        self.list_simple(test3_dir_local, quiet=True)
        self.list_verbose(test1_dir_gfarm, quiet=True)
        self.list_verbose(test3_dir_local, quiet=True)

        if readonly:
            # extract a member (SEE ALSO: test_prepare_srcdir)
            member = testsrc_name + '/dir1/readonly/file#2'
            all_members = [testsrc_name,
                           testsrc_name + '/dir1',
                           testsrc_name + '/dir1/readonly',
                           member,
                           ]
            test_member_g_name = 'test-gfptar-member-g'
            test_member_l_name = 'test-gfptar-member-l'
            test_member_g = workdir_gfarm_url.url_join(test_member_g_name)
            test_member_l = workdir_local_url.url_join(test_member_l_name)
            self.extract(test_member_g, test1_dir_gfarm, [member])
            self.extract(test_member_l, test3_dir_local, [member])
            g_member = GfURL.init(os.path.join(test_member_g, member))
            l_member = GfURL.init(os.path.join(test_member_l, member))
            if not g_member.compare_data(l_member):
                logger.error_exit(1, testname
                                  + '(extract a member) ' +
                                  '... FAIL (data mismatch)')
            for testdir in [test_member_g, test_member_l]:
                url = GfURL.init(testdir)
                for ent in url.listdir(recursive=True):
                    p = ent.subpath(url)
                    if p in all_members:
                        continue
                    logger.error('unexpected member: %s', p)
                    logger.error_exit(1, testname + '(extract a member) ' +
                                      '... FAIL (unexpected member found)')

        test5_name = 'test-5-create'
        test5_dir_gfarm = workdir_gfarm_url.url_join(test5_name)
        test6_name = 'test-6-extract'
        test6_dir_local = workdir_local_url.url_join(test6_name)

        # NOTE: check ignoring hardlinks for Local
        # Local -> Gfarm(tar)
        try:
            self.create(test5_dir_gfarm, workdir_local, [testsrc_name])
            if noread:
                logger.error_exit(1, testname + '(create:Local->Gfarm) ' +
                                  '... FAIL (unexpected success)')
        except Exception:
            if not noread:
                raise
        # Gfarm(tar) -> Local
        self.extract(test6_dir_local, test5_dir_gfarm, [])

        test4_srcdir_local = os.path.join(test4_dir_local, testsrc_name)
        if not self.test_compare_local(test4_srcdir_local, srcdir_local,
                                       same_owner=True):
            logger.error_exit(1, testname + ' ... FAIL (different data)(1)')

        if self.test_compare_local(test4_dir_local, test6_dir_local,
                                   same_owner=True):
            print(testname + ' ... PASS')
            workdir_local_url.remove_tree(remove_readonly=True)
            workdir_gfarm_url.remove_tree(remove_readonly=True)
        else:
            logger.error_exit(1, testname + ' ... FAIL (different data)(2)')

    def test_member(self):
        testname = 'gfptar-test-member'
        d_name = f'{testname}-{self.gfarm_user}-{self.uid}-{self.pid}'
        workdir_local = os.path.join(self.opt['--test-workdir-local'], d_name)
        workdir_gfarm = os.path.join(self.opt['--test-workdir-gfarm'], d_name)
        workdir_local_url = GfURL.init(workdir_local)
        workdir_gfarm_url = GfURL.init(workdir_gfarm)
        workdir_local_url.mkdir()
        workdir_gfarm_url.mkdir()

        testsrc_name = 'test-src'
        srcdir_local = workdir_local_url.url_join(testsrc_name)
        srcdir_gfarm = workdir_gfarm_url.url_join(testsrc_name)
        self.test_prepare_srcdir(srcdir_local, readonly=True)
        self.test_prepare_srcdir(srcdir_gfarm, readonly=True)

        test1_name = 'test-1-create'
        test1_dir_gfarm = workdir_gfarm_url.url_join(test1_name)
        test2_name = 'test-2-extract'
        test2_dir_gfarm = workdir_gfarm_url.url_join(test2_name)
        test3_name = 'test-3-create'
        test3_dir_local = workdir_local_url.url_join(test3_name)
        test4_name = 'test-4-extract'
        test4_dir_local = workdir_local_url.url_join(test4_name)

        # pick files as members (SEE ALSO: test_prepare_srcdir)
        files = ['file1', 'dir1/readonly/file#2']

        # Gfarm -> Gfarm(tar)
        self.create(test1_dir_gfarm, srcdir_gfarm, files)
        # Gfarm(tar) -> Gfarm
        self.extract(test2_dir_gfarm, test1_dir_gfarm, files)

        # Local -> Local(tar)
        self.create(test3_dir_local, srcdir_local, files)
        # Local(tar) -> Local
        self.extract(test4_dir_local, test3_dir_local, files)

        for f in files:
            g_member = GfURL.init(os.path.join(test2_dir_gfarm, f))
            l_member = GfURL.init(os.path.join(test4_dir_local, f))
            if not g_member.compare_data(l_member):
                logger.error_exit(1, testname
                                  + '... FAIL (data mismatch)')
        print(testname + ' ... PASS')
        workdir_local_url.remove_tree(remove_readonly=True)
        workdir_gfarm_url.remove_tree(remove_readonly=True)

    def test_prepare_srcdir(self, dir_url_str,
                            readonly=False, noread=False,
                            link=False, longname=False):
        logger.debug(f'readonly={str(readonly)}, noread={str(noread)}, '
                     f'link={str(link)}, longname={str(longname)}')
        F = 'file'
        D = 'directory'
        S = 'symlink'
        L = 'hardlink'
        longname = ('0123456789' * 30)[:255]
        tree = [
            (F, 'file1', 0o664, 1234567890, None),
            (D, 'dir1', 0o715, 2234567890, None),
        ]
        tree_readonly = [
            (D, 'dir1/readonly', 0o700, 3234567891, None),  # chmod 500 later
            (F, 'dir1/readonly/file#2', 0o456, 4234567892, None),
        ]
        tree_noread = [
            (F, 'dir1/noread-file', 0o000, 1234567891, None),
            (D, 'dir1/noread-dir', 0o700, 1234567892, None),  # chmod 000 later
            (F, 'dir1/noread-dir/file3', 0o400, 1234567893, None),
        ]
        tree_link = [
            (D, 'dir1/ 2', 0o755, 5234567890, None),
            (L, 'dir1/ 2/hardlink1',
             0o400, 6234567890, 'file1'),
            (S, 'dir1/ 2/symlink1',
             0o777, 7234567890, 'hardlink1'),
        ]
        tree_longname = [
            (F, 'dir1/' + longname, 0o775, 9234567890, None),
        ]
        if readonly:
            tree += tree_readonly
        if noread:
            tree += tree_noread
        if link:
            tree += tree_link
        if longname:
            tree += tree_longname
        srcdir_url = GfURL.init(dir_url_str)
        srcdir_url.mkdir()
        for ent in tree:
            ftype = ent[0]
            path = ent[1]
            mode = ent[2]
            mtime = ent[3]
            linkname = ent[4]
            url = GfURL.init(srcdir_url.url_join(path))
            if ftype == F:
                with url.writeopen(textmode=True,
                                   mode=mode,
                                   mtime=mtime,
                                   use_fsync=self.use_fsync) as f:
                    f.write(path)
            elif ftype == D:
                url.mkdir()
            elif ftype == S:
                url.symlink(linkname)
                url.chmod(mode, mtime=mtime, follow_symlinks=False)
            elif ftype == L:
                url.hardlink(srcdir_url.url_join(linkname))
                url.chmod(mode, mtime=mtime)
        for ent in reversed(tree):
            ftype = ent[0]
            path = ent[1]
            mode = ent[2]
            mtime = ent[3]
            # linkname = ent[4]
            url = GfURL.init(srcdir_url.url_join(path))
            if ftype == D:
                url.chmod(mode, mtime=mtime)
        srcdir_url.chmod(mode=0o700, mtime=0)
        if readonly:
            readonly_url = GfURL.init(srcdir_url.url_join('dir1/readonly'))
            readonly_url.chmod(mode=0o500)
        if noread:
            noread_url = GfURL.init(srcdir_url.url_join('dir1/noread-dir'))
            noread_url.chmod(mode=0o000)

    def test_specified_dir(self):
        basedir = self.opt['--basedir']
        if basedir is None:
            return
        infiles = self.opt['<member>']
        if len(infiles) == 0:
            return

        basedir_url = GfURL.init(basedir)

        testname = 'gfptar-test-specified-dir'
        d_name = f'{testname}-{self.gfarm_user}-{self.uid}-{self.pid}'
        workdir_local = os.path.join(self.opt['--test-workdir-local'], d_name)
        workdir_gfarm = os.path.join(self.opt['--test-workdir-gfarm'], d_name)
        workdir_local_url = GfURL.init(workdir_local)
        workdir_gfarm_url = GfURL.init(workdir_gfarm)
        workdir_local_url.mkdir()
        workdir_gfarm_url.mkdir()

        test9_name = 'test-9-create'
        test9_dir_gfarm = workdir_gfarm_url.url_join(test9_name)
        test10_name = 'test-10-extract'
        test10_dir_gfarm = workdir_gfarm_url.url_join(test10_name)
        test11_name = 'test-11-create'
        test11_dir_local = workdir_local_url.url_join(test11_name)
        test12_name = 'test-12-extract'
        test12_dir_local = workdir_local_url.url_join(test12_name)

        # basedir -> Gfarm(tar)
        self.create(test9_dir_gfarm, basedir, infiles)
        # Gfarm(tar) -> Gfarm
        self.extract(test10_dir_gfarm, test9_dir_gfarm, [])
        # Gfarm -> Local(tar)
        self.create(test11_dir_local, test10_dir_gfarm, infiles)
        # Local(tar) -> Local
        self.extract(test12_dir_local, test11_dir_local, [])

        result = True
        for infile in infiles:
            src = basedir_url.url_join(infile)
            result_dir = os.path.join(test12_dir_local, infile)
            # owner on local and Gfarm may be different
            same_owner = not basedir_url.is_gfarm()
            if not self.test_compare(result_dir, src, data=True,
                                     same_owner=same_owner):
                result = False
                break
        if result:
            print(testname + ' ... PASS')
            workdir_local_url.remove_tree(remove_readonly=True)
            workdir_gfarm_url.remove_tree(remove_readonly=True)
        else:
            logger.error_exit(1, testname + ' ... FAIL (different data)')

    def test_compare_local(self, dir1, dir2, data=False, same_owner=False):
        # SEE ALSO: test_prepare_srcdir
        exclude_list = [r'noread-*']
        diff_args = ['diff']
        for exclude in exclude_list:
            diff_args += ['--exclude', exclude]
        diff_args += ['-r', dir1, dir2]
        out, err, ret = execcmd_raw(diff_args)
        if ret != 0:
            logger.error(f'diff -r {dir1} {dir2}: {err}')
            return False
        logger.debug('diff -r (data check): PASS')
        return self.test_compare(dir1, dir2, data=data, same_owner=same_owner,
                                 exclude_list=exclude_list)

    def test_compare(self, dir1, dir2, data=True, same_owner=False,
                     exclude_list=None):
        # compare mtime and mode of dir1/files with dir2/files
        d1url = GfURL.init(dir1)
        d2url = GfURL.init(dir2)
        return d1url.compare_entries(d2url, data=True, same_owner=same_owner,
                                     exclude_list=exclude_list)

    def MT_enabled(self):
        return self.jobs >= 2

    # lock required
    def info(self, fmt, *args):
        if self.verbose:
            print(fmt.format(*args))

    def create(self, outdir, basedir, infiles):
        self.options_init()
        self.outdir = outdir
        self.outdir_url = GfURL.init(outdir)
        self.basedir_url = GfURL.init(basedir)
        self.size = self.opt['--size']
        self.ratio = self.opt['--ratio']
        self.compress_type = self.opt['--type']
        self.compress_prog = self.opt['--use-compress-program']
        self.disable_gfarm_command = self.opt['--disable-gfarm-command']
        self.use_gfarm_command = not self.disable_gfarm_command
        self.gfsched_interval = self.opt['--gfsched-interval']
        self.use_tqdm = self.opt['--use-tqdm']
        if self.compress_type == GfTarFile.COMPRESS_TYPE_NO:
            self.split_size = self.size
            self.suffix = '.tar'
        else:
            self.split_size = self.size * 100 / self.ratio
            self.suffix = '.tar.' + self.compress_type

        def entry_key(entry):
            return entry.path

        self.filelistlist = []
        self.total_size = 0
        self.total_num = 0
        self.start_time = time.time()
        self.next_time = self.start_time + 1

        has_error = None
        filelist = []
        for infile in infiles:
            infile_url = GfURL.init(infile)
            if not infile_url.is_local():
                raise Exception('specifying a relative path is required '
                                'instead of a URL: ' + infile)
            infile = infile_url.path  # normalize and ignore scheme
            infile = infile.lstrip('/')  # relative path only
            if infile.startswith('./'):
                infile = infile[2:]
            if infile == '' or infile == '.':
                raise Exception('specifying current directory (.) '
                                + 'as members is not allowed: ' + infile)
            if infile == '..' or infile.startswith('../'):
                raise Exception('specifying parent directory (..) '
                                + 'as members is not allowed: ' + infile)
            url_str = os.path.join(self.basedir_url.url_str, infile)
            gfurl = GfURL.init(url_str)
            logger.debug('listdir: %s', gfurl.url_str)
            try:
                for entry in gfurl.listdir(recursive=True, first=True,
                                           hardlink_warn=self.hardlink_warn):
                    logger.debug('listdir: entry.path=%s', entry.path)
                    filelist.append(entry)
                    self.total_size += entry.size
                    self.total_num += 1
                    if self.progress_enabled:
                        now = time.time()
                        if now >= self.next_time:
                            self.next_time = now + 1
                            self.progress_for_list1(now)
            except Exception as e:
                if has_error is None:  # save first error
                    has_error = e
                logger.debug(str(e))
                # continue

        filelist.sort(key=entry_key, reverse=False)
        self.filelistlist = self.schedule(filelist)
        if self.progress_enabled:
            self.progress_for_list1(time.time())
            sys.stdout.write('\n')

        self.tqdm = None
        self.myprogress = False
        if self.progress_enabled:
            if self.use_tqdm and have_tqdm:
                term_size = shutil.get_terminal_size()
                # bar_format = '{l_bar}{r_bar}'
                bar_format = '{percentage:3.0f}% {n_fmt}/{total_fmt}' \
                    + ' [{elapsed}<{remaining}, {rate_fmt}]'
                self.tqdm = tqdm(total=self.total_size, unit_scale=True,
                                 unit='B', dynamic_ncols=False,
                                 bar_format=bar_format,
                                 ncols=int(term_size.columns*3/4))
            else:
                self.myprogress = True

        self.serial = 0
        self.archive_size = 0
        self.stored_size = 0
        self.stored_num = 0
        self.start_time = time.time()
        self.next_time = self.start_time + 1

        self.gfsched_lock = None
        self.gfsched_next = 0
        self.gfsched_list = None

        self.outdir_url.create_new_dir()
        try:
            if self.MT_enabled():
                self.create_tars_MT()
            else:
                self.create_tars()
        except Exception as e:
            if has_error is None:  # save first error
                has_error = e
            logger.debug(str(e))

        if self.myprogress:
            self.progress_for_create(time.time())
            sys.stdout.write('\n')
        if self.tqdm:
            self.tqdm.close()
        if not self.quiet and self.stored_size > 0:
            print('compression ratio: %.2f %% (%d/%d)' %
                  (100 * self.archive_size / self.stored_size,
                   self.archive_size, self.stored_size))
        del self.filelistlist
        if self.is_canceled():
            raise self.error_canceled()
        if has_error is not None:
            raise has_error

    def create_tars(self):
        self.lock_init(False)
        has_error = None
        for filelist in self.filelistlist:
            self.serial += 1
            try:
                self.create_a_tar(self.serial, filelist)
            except Exception as e:
                if has_error is None:  # save first error
                    has_error = e
                    logger.debug(str(e))
        if has_error is not None:
            raise has_error

    def create_tars_MT(self):
        self.lock_init(True)
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.jobs) as executor:
            futures = {}
            for filelist in self.filelistlist:
                self.serial += 1
                t = executor.submit(self.create_a_tar, self.serial, filelist)
                futures[t] = self.serial
            self.futures = futures
            has_error = None
            for t in concurrent.futures.as_completed(futures, timeout=None):
                exc = t.exception()
                if exc:
                    logger.debug('%s: serial=%04d', repr(exc), futures[t])
                    if self.verbose or self.debug:
                        tb = traceback.TracebackException.from_exception(exc)
                        logger.debug(''.join(tb.format()))
                    if has_error is None:  # save first error
                        has_error = exc
            if has_error is not None:
                raise has_error

    def schedule(self, filelist):
        filelistlist = []
        newlist = []
        total = 0
        for entry in filelist:
            total += entry.size
            if total >= self.split_size:
                if len(newlist) > 0:
                    filelistlist.append(newlist)
                newlist = []
                total = entry.size
            newlist.append(entry)
        if len(newlist) > 0:
            filelistlist.append(newlist)
        return filelistlist

    def lock_init(self, enable, timeout=None):
        if enable:
            if timeout is not None:
                self._lock = threading.Lock()
                self._lock_timeout = timeout
                self.lock = self.lock_timeout
            else:
                self._lock = None
                self.lock = threading.Lock
        else:
            self._lock = None
            self.lock = self.lock_noop

    @contextmanager
    def lock_noop(self):
        yield

    # NOTE: not used
    @contextmanager
    def lock_timeout(self):
        locked = True
        if not self._lock.acquire(timeout=self._lock_timeout):
            logger.warning('lock: timeout(unexpected)')
            locked = False
        try:
            yield
        finally:
            if locked:
                self._lock.release()

    def cancel(self):
        with self.lock():
            self.canceled.set()

    def is_canceled(self):
        with self.lock():
            val = self.canceled.is_set()
        return val

    def _gfsched_sometimes(self, gfurl):
        # if gfurl.is_gfarm() is not True:
        #     raise AssertionError
        if self.gfsched_lock is None:
            self.gfsched_lock = threading.Lock()
        with self.gfsched_lock:
            now = time.time()
            if now >= self.gfsched_next:
                # access to Gfarm
                self.gfsched_list = gfurl.gfsched(write_mode=True,
                                                  number=self.jobs)
                # prevent IndexError
                self.gfsched_list = self.gfsched_list[:self.jobs]
                logger.debug("update gfsched_list")
                self.gfsched_next = now + self.gfsched_interval

    def select_a_target_host(self, outurl, index):
        if not outurl.is_gfarm():
            return None
        if self.jobs <= 1:
            return None
        self._gfsched_sometimes(outurl)
        with self.lock():
            target_host = self.gfsched_list[index % len(self.gfsched_list)]
        logger.debug("selected target_host: %s", target_host)
        return target_host

    def create_a_tar(self, serial, filelist):
        try:
            self.create_a_tar0(serial, filelist)
        except Exception:
            if self.is_canceled():
                raise self.error_canceled()
            else:
                raise

    def create_a_tar0(self, serial, filelist):
        logger.debug('create_a_tar: start: %04d', serial)
        if self.is_canceled():
            logger.debug('canceled (1): serial=%04d', serial)
            return
        first = None
        last = None
        for entry in filelist:
            if entry.is_file():
                first = entry
                break
        if first is None:
            first = filelist[0]
        for entry in reversed(filelist):
            if entry.is_file():
                last = entry
                break
        if last is None or first == last:
            firstpath = first.subpath(self.basedir_url)
            outname = '%s%s' % (firstpath, self.suffix)
        else:
            firstpath = first.subpath(self.basedir_url)
            lastpath = last.subpath(self.basedir_url)
            outname = '%s..%s%s' % (firstpath, lastpath, self.suffix)

        serial_str = '%04d_' % serial
        outname_max = GfURL.MAXNAMLEN - len(serial_str) - len(self.LIST_SUFFIX)
        if len(outname) > outname_max:
            # use last half of name
            outname = outname[-outname_max:]
        # ex.: home/user1/dir -> home_user1_dir
        outname = serial_str + outname.replace('/', '_')
        outurl = GfURL.init(self.outdir_url.url_join(outname),
                            use_gfarm_command=self.use_gfarm_command)
        target_host = self.select_a_target_host(outurl, serial)
        tar = GfTarFile.create_open(outurl, self.compress_type, self.bufsize,
                                    compress_prog=self.compress_prog,
                                    use_fsync=self.use_fsync,
                                    target_host=target_host)
        has_error = None
        filelist_ok = []
        for entry in filelist:
            if self.is_canceled():
                logger.debug('canceled (2): serial=%04d', serial)
                break
            subpath = entry.subpath(self.basedir_url)
            try:
                tar.add_entry(subpath, entry)
                filelist_ok.append(entry)
                with self.lock():
                    self.info('stored: {}', subpath)
                    self.stored_size += entry.size
                    if self.myprogress:
                        self.stored_num += 1
                        now = time.time()
                        if now >= self.next_time:
                            self.next_time = now + 1
                            self.progress_for_create(now)
                    elif self.tqdm:
                        if entry.is_file():
                            self.tqdm.update(entry.size)
            except Exception as e:
                if has_error is None:  # save first error
                    has_error = e
                logger.warning(str(e))
                # continue

        tar.close()
        tar_size = outurl.get_size()
        self.create_a_members_list(outurl, filelist_ok, target_host)
        with self.lock():
            self.info('created(.tar): {}', outurl.url_str)
            self.archive_size += tar_size
        if has_error is not None:
            raise has_error

    def create_a_members_list(self, url, filelist, target_host):
        outurl = GfURL.init(url.url_str + self.LIST_SUFFIX)
        with outurl.writeopen(textmode=True, use_fsync=self.use_fsync,
                              hostname=target_host) as f:
            for entry in filelist:
                # ex. "D /path/to/dir"
                if entry.is_file():
                    f.write('F ')
                elif entry.is_directory():
                    f.write('D ')
                elif entry.is_symlink():
                    f.write('S ')
                else:  # unknown
                    f.write('? ')
                subpath = entry.subpath(self.basedir_url)
                f.write(subpath)
                f.write('\n')
        with self.lock():
            self.info('created(.lst): {}', outurl.url_str)

    def error_canceled(self):
        return Exception('Canceled')

    def error_not_a_gfptar_directory(self, url_str):
        return Exception('Not a gfptar-archived directory: ' + url_str)

    def extract(self, outdir, indir, members):
        self.options_init()
        self.outdir = outdir
        self.outdir_url = GfURL.init(outdir)
        self.indir = indir
        member_set = set(members)
        self.compress_prog = self.opt['--use-compress-program']
        self.same_owner = self.opt['--same-owner']
        self.disable_gfarm_command = self.opt['--disable-gfarm-command']
        self.use_gfarm_command = not self.disable_gfarm_command
        self.gfsched_interval = self.opt['--gfsched-interval']

        indir_url = GfURL.init(self.indir)
        if not indir_url.exists():
            raise FileNotFoundError(indir_url.url_str)
        if not indir_url.is_directory():
            raise self.error_not_a_gfptar_directory(indir_url.url_str)

        search_target = len(member_set) > 0
        archive_dict = {}  # member -> (file_type, tar filename)
        target_set_all = set()  # all tar files
        directory_list = []
        self.total_num = 0
        self.start_time = time.time()
        self.next_time = self.start_time + 1

        for ent in indir_url.listdir(recursive=False):
            if ent.path.endswith(self.LIST_SUFFIX):  # ignored
                continue
            if self.is_canceled():
                raise self.error_canceled()
            subpath = ent.subpath(indir_url)
            arch_url_str = indir_url.url_join(subpath)
            target_set_all.add(arch_url_str)
            list_url = GfURL.init(arch_url_str + self.LIST_SUFFIX)
            if not list_url.exists():
                raise self.error_not_a_gfptar_directory(indir_url.url_str)
            with list_url.readopen(textmode=True) as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    if self.is_canceled():
                        raise self.error_canceled()
                    self.total_num += 1
                    line = line.rstrip()
                    # ex. "D /path/to/dir"
                    file_type = line[:1]
                    path = line[2:].lstrip('/')
                    if search_target:
                        archive_dict[path] = (file_type, arch_url_str)
                    else:
                        if file_type == 'D':
                            directory_list.append(path)
            if self.progress_enabled:
                now = time.time()
                if now >= self.next_time:
                    self.next_time = now + 1
                    self.progress_for_list2(now)
        if self.progress_enabled:
            self.progress_for_list2(time.time())
            sys.stdout.write('\n')

        if search_target:
            # selected tar files
            target_set = set()
            directory_set = set()
            new_member_set = set()
            for member in member_set:
                m = GfURL.init(member, local=True)
                new_member_set.add(m.path)  # normalized
            member_set = new_member_set

            for member in member_set:
                type_arch = archive_dict.get(member, None)
                if type_arch is None:
                    raise Exception('Not found in archive: ' + member)
                file_type, target_arch = type_arch
                target_set.add(target_arch)
                if file_type == 'D':
                    directory_set.add(member)

            # add parent directories to update attributes (mode,mtime)
            new_member_set = set()
            for member in member_set:
                new_member_set.add(member)
                url = GfURL.init(member, local=True)  # relative path
                for parent_url in url.parent_list:
                    path = parent_url.path
                    if path != '.' and path != '/':
                        type_arch = archive_dict.get(path, None)
                        if type_arch is not None:  # found
                            file_type, target_arch = type_arch
                            target_set.add(target_arch)
                            directory_set.add(path)
                            new_member_set.add(path)
            del member_set
            member_set = new_member_set  # replace
            self.total_num = len(member_set)  # re-set
            del archive_dict
            directory_list = list(directory_set)
            del directory_set
        else:
            target_set = target_set_all

        target_list = list(target_set)
        del target_set
        target_list.sort()

        self.outdir_url.create_new_dir()

        self.created_directory_set = set()
        # self.extract_directories(directory_list)
        self.extract_directories_fast(directory_list)

        self.extracted_num = 0
        self.extracted_size = 0
        self.start_time = time.time()
        self.next_time = self.start_time + 1
        self.dirstat_dict = {}

        self.gfsched_lock = None
        self.gfsched_next = 0
        self.gfsched_list = None

        if self.MT_enabled():
            self.extract_from_archives_MT(target_list, member_set)
        else:
            self.extract_from_archives(target_list, member_set)
        if self.progress_enabled:
            self.progress_for_extract(time.time())
            sys.stdout.write('\n')

        self.update_stat_for_directories(directory_list, member_set)

    def extract_directories(self, directory_list):
        # NOTE: slow on Gfarm
        directory_list.sort()
        for d in directory_list:
            url_str = self.outdir_url.url_join(d)
            dir_url = GfURL.init(url_str)
            try:
                dir_url.mkdir()
            except Exception:
                dir_url.makedirs()
            self.info('prepare_dir: {}', dir_url.url_str)
            self.created_directory_set.add(dir_url.path)

    def extract_directories_fast(self, directory_list):
        # faster implementation for gfmkdir
        directory_list.sort(reverse=True)
        created = self.created_directory_set
        for d in directory_list:
            url_str = self.outdir_url.url_join(d)
            dir_url = GfURL.init(url_str)
            # url.path is normalized
            if dir_url.path in created:
                logger.debug('skip (already created): %s', url_str)
                continue
            parent_url = dir_url.parent
            if parent_url.path in created:
                dir_url.mkdir()
                created.add(dir_url.path)
                self.info('prepare_dir: {}', dir_url.path)
            else:  # no parent
                dir_url.makedirs()
                created.add(dir_url.path)
                self.info('prepare_dir: {}', dir_url.path)
                created.add(parent_url.path)
                self.info('prepare_dir: {}', parent_url.path)
                for p in parent_url.parent_list:
                    path = p.path
                    if path == '.' or path == '/':
                        continue
                    created.add(path)
                    self.info('prepare_dir: {}', path)

    def update_stat_for_directories(self, directory_list, member_set):
        # process from leaves
        directory_list.sort(reverse=True)
        members_num = len(member_set)
        for d in directory_list:
            if members_num > 0 and d not in member_set:
                continue
            tarinfo = self.dirstat_dict.get(d)
            if tarinfo is None:
                logger.warning('No information of the directory: %s', d)
                continue
            url_str = self.outdir_url.url_join(d)
            dir_url = GfURL.init(url_str)
            if self.same_owner:
                dir_url.chmod(tarinfo.mode, mtime=tarinfo.mtime,
                              user=tarinfo.uname, group=tarinfo.gname)
            else:
                dir_url.chmod(tarinfo.mode, mtime=tarinfo.mtime)
            logger.debug("update_stat: %s, %s", d, oct(tarinfo.mode))
            self.info('update_stat: {}', d)

    def extract_from_archives(self, target_list, member_set):
        self.lock_init(False)
        serial = 0
        for target in target_list:
            logger.debug('target_set: %s', target)
            serial += 1
            self.extract_from_a_tar(serial, target, member_set)

    def extract_from_archives_MT(self, target_list, member_set):
        self.lock_init(True)
        with concurrent.futures.ThreadPoolExecutor(
                max_workers=self.jobs) as executor:
            futures = {}
            serial = 0
            for target in target_list:
                logger.debug('target_set: %s', target)
                serial += 1
                t = executor.submit(self.extract_from_a_tar,
                                    serial, target, member_set)
                futures[t] = target

            for t in concurrent.futures.as_completed(futures, timeout=None):
                exc = t.exception()
                if exc:
                    logger.error('%s: %s', repr(exc), futures[t])
                    tb = traceback.TracebackException.from_exception(exc)
                    logger.debug(''.join(tb.format()))
                    if not self.is_canceled():
                        self.cancel()
                        for t2 in futures:
                            logger.error("cancel: name=%s", futures[t2])
                            t2.cancel()
                    raise exc

    def extract_from_a_tar(self, serial, target, member_set):
        try:
            self.extract_from_a_tar0(serial, target, member_set)
        except Exception:
            if self.is_canceled():
                raise self.error_canceled()
            else:
                raise

    def extract_from_a_tar0(self, serial, target, member_set):
        logger.debug('extract_from_a_tar: start: %04d', serial)
        if self.is_canceled():
            logger.debug('canceled (1): name=%s', target)
            return
        arch_url = GfURL.init(target,
                              use_gfarm_command=self.use_gfarm_command)
        tar = GfTarFile.extract_open(arch_url, self.bufsize,
                                     compress_prog=self.compress_prog)
        members_num = len(member_set)
        index = serial
        while True:
            if self.is_canceled():
                logger.debug('canceled (2): name=%s', target)
                break
            try:
                tarinfo = tar.next()
            except Exception as e:
                logger.warning(f'SKIPPED: invalid or empty tar ({target}):'
                               f' {str(e)}')
                tarinfo = None
            if tarinfo is None:
                break
            if members_num > 0:
                if tarinfo.name not in member_set:
                    continue  # not a target
            # members_num == 0 -> extract all

            # ex. /a/b/c/ -> a/b/c
            outfile = tarinfo.name.strip('/')  # relative path only
            url_str = self.outdir_url.url_join(outfile)
            outurl = GfURL.init(url_str)

            parent = outurl.parent
            with self.lock():
                exist_dir = parent.path in self.created_directory_set
            if not exist_dir:
                if not parent.exists():
                    parent.makedirs()
                with self.lock():
                    self.created_directory_set.add(parent.path)

            if tarinfo.isfile():
                target_host = self.select_a_target_host(outurl, index)
                index += 1
                user = None
                group = None
                if self.same_owner:
                    user = tarinfo.uname
                    group = tarinfo.gname
                inf = tar.extractfile(tarinfo)  # io.BufferedReader
                size = outurl.copy_from(inf, self.bufsize,
                                        mode=tarinfo.mode,
                                        mtime=tarinfo.mtime,
                                        user=user, group=group,
                                        use_fsync=self.use_fsync,
                                        hostname=target_host)
                inf.close()
                logger.debug('extract,file: %s, %d', outfile, size)
                with self.lock():
                    self.info('extracted(F): {}', outfile)
            elif tarinfo.isdir():
                # NOTE: already created
                logger.debug('extract,dir: %s', outfile)
                with self.lock():
                    self.dirstat_dict[outfile] = tarinfo
            elif tarinfo.issym():
                logger.debug('extract,link: %s, %s', outfile,
                             tarinfo.linkname)
                outurl.symlink(tarinfo.linkname)
                if self.same_owner:
                    outurl.chmod(tarinfo.mode, mtime=tarinfo.mtime,
                                 user=tarinfo.uname, group=tarinfo.gname,
                                 follow_symlinks=False)
                else:
                    outurl.chmod(tarinfo.mode, mtime=tarinfo.mtime,
                                 follow_symlinks=False)
                with self.lock():
                    self.info('extracted(S): {}', outfile)
            else:
                logger.warning('unsupported type: %s: %s',
                               outfile, tarinfo.type)
            if self.progress_enabled:
                with self.lock():
                    self.extracted_num += 1
                    if tarinfo.isfile():
                        self.extracted_size += tarinfo.size
                    now = time.time()
                    if now >= self.next_time:
                        self.next_time = now + 1
                        self.progress_for_extract(now)

        tar.close()
        with self.lock():
            self.info('extracted(done): {}', arch_url.url_str)

    def progress_for_list1(self, now):
        sec = now - self.start_time
        sys.stdout.write(f'\rlisting: '
                         f'num={self.total_num}, '
                         f'size={self.total_size}, '
                         f'sec={sec:.0f}')

    def progress_for_list2(self, now):
        sec = now - self.start_time
        sys.stdout.write(f'\rlisting: '
                         f'num={self.total_num}, '
                         f'sec={sec:.0f}')

    # lock required
    def progress_for_create(self, now):
        sec = now - self.start_time
        if self.total_size > 0:
            percent = self.stored_size * 100 / self.total_size
        else:
            percent = 0
        if sec > 0:
            bytes_per_sec = self.stored_size / sec
        else:
            bytes_per_sec = 0
        sys.stdout.write(f'\rcreated: {percent:.0f}%, '
                         f'num={self.stored_num}/{self.total_num}, '
                         f'size={self.stored_size}, '
                         f'sec={sec:.0f}, '
                         f'B/s={bytes_per_sec:.0f}  ')

    # lock required
    def progress_for_extract(self, now):
        sec = now - self.start_time
        if self.total_num > 0:
            percent = self.extracted_num * 100 / self.total_num
        else:
            percent = 0
        if sec > 0:
            bytes_per_sec = self.extracted_size / sec
        else:
            bytes_per_sec = 0
        sys.stdout.write(f'\rextracted: {percent:.0f}%, '
                         f'num={self.extracted_num}/{self.total_num}, '
                         f'size={self.extracted_size}, '
                         f'sec={sec:.0f}, '
                         f'B/s={bytes_per_sec:.0f}  ')

    def list_simple(self, indir, quiet=False):
        self.options_init()
        indir_url = GfURL.init(indir)
        filelistlist = []
        for ent in indir_url.listdir(recursive=False):
            if not ent.path.endswith(self.LIST_SUFFIX):
                continue
            filelistlist.append(ent.path)
        filelistlist.sort()
        for filelist in filelistlist:
            list_url = GfURL.init(filelist)
            with list_url.readopen(textmode=True) as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    if not quiet:
                        print(line.rstrip())

    def list_verbose(self, indir, quiet=False):
        self.options_init()
        self.compress_prog = self.opt['--use-compress-program']
        indir_url = GfURL.init(indir)
        archlist = []
        for ent in indir_url.listdir(recursive=False):
            if ent.path.endswith(self.LIST_SUFFIX):
                continue
            archlist.append(ent.path)
        archlist.sort()
        for path in archlist:
            arch_url = GfURL.init(path)
            tar = GfTarFile.extract_open(arch_url, self.bufsize,
                                         compress_prog=self.compress_prog)
            while True:
                try:
                    t = tar.next()
                except Exception as e:
                    logger.warning(f'SKIPPED: invalid or empty tar ({path}):'
                                   f' {str(e)}')
                    t = None
                if t is None:
                    break
                name = t.name
                if t.isdir():
                    name = name + '/'
                info = (f'{t.mode:4o} {t.uname:>10}/{t.gname:<10}'
                        f' {t.size:9d} {t.mtime} {name}')
                if not quiet:
                    print(info)


progname = os.path.basename(__file__)


__doc__ = """
Usage:
  {f} [options] -c <outdir> [-C <basedir>] [--] <member>...
  {f} [options] -x <outdir> [--] <indir> [<member>...]
  {f} [options] -t <indir>
  {f} [options] --test
  {f} [options] --test -C <basedir> <member>...
  {f} -h | --help

Options:
  -t, --list=DIR            list mode,
                            list the members of <indir>
  -x, --extract=DIR         extract mode,
                            extract all members or specified <member>s
                            from <indir> to <outdir>
  -c, --create=DIR          create mode,
                            create tar files in <outdir> from <member>s
  -C, --basedir=DIR         base directory for <member>s  [default: .]
  -j, --jobs=NUM            the number of jobs to copy per tar file in parallel
                            [default: 4]
  -s, --size=BYTES          assumed bytes per output file [default: 200M]
  -T, --type=TYPE           compress type (gz,bz2,xz,no) [default: gz]
  -r, --ratio=RATIO         assumed compression ratio (%) [default: 50]
  -I, --use-compress-program=COMMAND
                            filter data through COMMAND,
                            the command must accept -d option for decompression
  --same-owner              extract files with the same ownership
                            (for euid=0 on local, or gfarmroot on Gfarm)
  --disable-gfarm-command   disable the use of gfreg and gfexport
                            for tar files on gfarm2fs
  --disable-fsync           disable calling fsync() before close()
  --gfsched-interval=SEC    interval of updating candidate hosts to write
                            (for Gfarm URL only) [default: 120]
  --use-tqdm                use tqdm to show progress (for --create)
  --encoding=CODEC          codec for filename encoding
             (https://docs.python.org/3/library/codecs.html#standard-encodings)
                            [default: utf-8]
  --bufsize=BYTES           buffer size to copy [default: 1M]
  --test                    test mode (-q option is recommended)
  --test-workdir-local=DIR  local directory for test [default: /tmp]
  --test-workdir-gfarm=DIR  Gfarm directory for test [default: gfarm:/tmp]
  -q, --quiet               quiet messages
  -v, --verbose             verbose output
  -d, --debug               debug mode
  -?, -h, --help            show this help and exit

Example of --create (Gfarm to Gfarm):
  Command line:
    gfptar -c gfarm:/home/user1/out -C gfarm:/home/user1 ./dir
  Input files:
    gfarm:/home/user1/dir/test0000.data
    ...
    gfarm:/home/user1/dir/test9999.data
  Output files:
    gfarm:/home/user1/out/0001_dir_test0000.data..dir_test0999.data.tar.gz
    gfarm:/home/user1/out/0001_dir_test0000.data..dir_test0999.data.tar.gz.lst
    ...
    gfarm:/home/user1/out/0010_dir_test9000.data..dir_test9999.data.tar.gz
    gfarm:/home/user1/out/0010_dir_test9000.data..di1_test9999.data.tar.gz.lst
  Contents of list file (*.lst):
    F dir/test0000.data
    ...
    F dir/test0999.data

Example of --extract (Gfarm to Gfarm):
  Command line:
    gfptar -x gfarm:/home/user1/out2 gfarm:/home/user1/out
  Output files:
    gfarm:/home/user1/out2/dir/test0000.data
    ...
    gfarm:/home/user1/out2/dir/test9999.data

Limitations:
  - Hard links are not preserved.
  - File names cannot include newline characters.
  - Subsecond (less than a second) for mtime is not preserved.
""".format(f=progname)


_schema = Schema({
    '--list': Or(str, None),
    '--extract': Or(str, None),
    '--create': Or(str, None),
    '--basedir': Or(str, None),
    '--encoding': str,
    '--size': Use(unhumanize_number),
    '--bufsize': Use(unhumanize_number),
    '--type': str,
    '--ratio': Use(int),
    '--jobs': Use(int),
    '--use-compress-program': Or(str, None),
    '--disable-gfarm-command': bool,
    '--disable-fsync': bool,
    '--gfsched-interval': Use(int),
    '--same-owner': bool,
    '--use-tqdm': bool,
    '--test': bool,
    '--test-workdir-local': Or(str, None),
    '--test-workdir-gfarm': Or(str, None),
    '--quiet': bool,
    '--verbose': bool,
    '--debug': bool,
    '--help': bool,
    '--': bool,
    '<indir>': Or(str, None),
    '<member>': [str],
})


if __name__ == '__main__':
    gfptar = GfptarCommand(progname)
    gfptar.run()
